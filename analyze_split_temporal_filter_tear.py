#!/usr/bin/env python3
"""
æ’•è£‚é¢æ–‘å—æ•°é‡å’Œå¯†åº¦æ—¶é—´åºåˆ—åˆ†æï¼ˆä½¿ç”¨å‰ªåˆ‡é¢maskè¿‡æ»¤æ’•è£‚é¢ï¼‰
ä¸»è¦åŠŸèƒ½ï¼š
1. ä½¿ç”¨å‰ªåˆ‡é¢maskè¿‡æ»¤å‡ºæ’•è£‚é¢åŒºåŸŸ
2. ä»æ’•è£‚é¢åŒºåŸŸä¸­æå–æ–‘å—æ•°é‡å’Œå¯†åº¦æ•°æ®
3. ç»˜åˆ¶æ–‘å—æ•°é‡å’Œå¯†åº¦éšæ—¶é—´å˜åŒ–çš„æ›²çº¿å›¾
4. ç”Ÿæˆæ—¶é—´åºåˆ—åˆ†ææŠ¥å‘Š

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # åŸºæœ¬ä½¿ç”¨
    python analyze_split_temporal_filter_tear.py --roi_dir data/roi_imgs --output_dir output/tear_filter_analysis
    
    # æŒ‡å®šå¹³æ»‘å‚æ•°
    python analyze_split_temporal_filter_tear.py --roi_dir data/roi_imgs --output_dir output/tear_filter_analysis \\
        --smoothing_method gaussian --sigma 10.0 --window_size 50
    
    # æ§åˆ¶å¯è§†åŒ–ä¿å­˜é—´éš”ï¼ˆä¾‹å¦‚æ¯100å¸§ä¿å­˜ä¸€æ¬¡ï¼‰
    python analyze_split_temporal_filter_tear.py --roi_dir data/roi_imgs --output_dir output/tear_filter_analysis \\
        --viz_interval 100
    
    # è·³è¿‡ç¬¬ä¸€æ­¥ï¼ˆç¬¬ä¸€æ­¥å·²è¿è¡Œè¿‡ï¼‰
    python analyze_split_temporal_filter_tear.py --roi_dir data/roi_imgs --output_dir output/tear_filter_analysis \\
        --skip_step1
"""

import cv2
import numpy as np
import os
import glob
import sys
import argparse
import matplotlib.pyplot as plt
import pandas as pd
from typing import List, Dict, Any, Tuple
import json
import platform
from scipy import signal
from scipy.ndimage import gaussian_filter1d
from tqdm import tqdm

# æ·»åŠ data_processç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'data_process'))
# æ·»åŠ data_shear_splitç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'data_shear_split'))

from feature_extractor import FeatureExtractor
from config import PREPROCESS_CONFIG
from shear_tear_detector import ShearTearDetector
from spot_processor import SpotProcessor

# è®¾ç½®ä¸­æ–‡å­—ä½“
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    import matplotlib.font_manager as fm
    
    system = platform.system()
    
    if system == "Darwin":  # macOS
        available_fonts = [f.name for f in fm.fontManager.ttflist]
        chinese_fonts = []
        
        preferred_fonts = ['PingFang SC', 'Hiragino Sans GB', 'STHeiti', 'Arial Unicode MS']
        
        for font in preferred_fonts:
            if font in available_fonts:
                chinese_fonts.append(font)
        
        if chinese_fonts:
            plt.rcParams['font.sans-serif'] = chinese_fonts + ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            print(f"è®¾ç½®ä¸­æ–‡å­—ä½“: {chinese_fonts[0]}")
            return True
    
    elif system == "Windows":
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        print("è®¾ç½®Windowsä¸­æ–‡å­—ä½“")
        return True
    
    else:  # Linux
        plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'Droid Sans Fallback', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        print("è®¾ç½®Linuxä¸­æ–‡å­—ä½“")
        return True
    
    print("æ— æ³•è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
    return False

class TearFilterTemporalAnalyzer:
    """æ’•è£‚é¢è¿‡æ»¤æ—¶é—´åºåˆ—åˆ†æå™¨"""
    
    def __init__(self, viz_interval=None, skip_step1=False):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            viz_interval: å¯è§†åŒ–ä¿å­˜é—´éš”ï¼ˆNoneè¡¨ç¤ºä¿å­˜æ‰€æœ‰å¸§ï¼‰
            skip_step1: æ˜¯å¦è·³è¿‡ç¬¬ä¸€æ­¥çš„å¯è§†åŒ–ä¿å­˜ï¼ˆä»ä¼šè¿›è¡Œæ£€æµ‹ä»¥æ„å»ºç¼“å­˜ï¼‰
        """
        self.feature_extractor = FeatureExtractor(PREPROCESS_CONFIG)
        self.shear_tear_detector = ShearTearDetector()
        self.spot_processor = SpotProcessor()
        self.data = []
        self.viz_interval = viz_interval
        self.skip_step1 = skip_step1
        
    def extract_frame_info(self, filename: str) -> int:
        """ä»æ–‡ä»¶åæå–å¸§å·"""
        try:
            basename = os.path.basename(filename)
            # æå–frame_XXXXXXä¸­çš„æ•°å­—
            frame_num = int(basename.split('_')[1])
            return frame_num
        except (IndexError, ValueError):
            return -1
    
    def filter_tear_region_with_shear_mask(self, roi_image, segmented_image):
        """
        ä½¿ç”¨å‰ªåˆ‡é¢maskè¿‡æ»¤å‡ºæ’•è£‚é¢åŒºåŸŸ
        
        Args:
            roi_image: åŸå§‹ROIå›¾åƒ
            segmented_image: åˆ†å‰²ç»“æœå›¾åƒï¼ˆ128=æ’•è£‚é¢ï¼Œ255=å‰ªåˆ‡é¢ï¼‰
            
        Returns:
            è¿‡æ»¤åçš„æ’•è£‚é¢åŒºåŸŸå›¾åƒ
        """
        # ç¡®ä¿åˆ†å‰²å›¾åƒå’ŒåŸå›¾å°ºå¯¸ä¸€è‡´
        if roi_image.shape != segmented_image.shape:
            # å¦‚æœå°ºå¯¸ä¸ä¸€è‡´ï¼Œå°†åˆ†å‰²å›¾åƒresizeåˆ°ä¸åŸå›¾ç›¸åŒçš„å°ºå¯¸
            segmented_image = cv2.resize(segmented_image, 
                                        (roi_image.shape[1], roi_image.shape[0]), 
                                        interpolation=cv2.INTER_NEAREST)
        
        # ä»åˆ†å‰²ç»“æœä¸­æå–å‰ªåˆ‡é¢maskï¼ˆå€¼ä¸º255çš„åŒºåŸŸï¼‰
        shear_mask = (segmented_image == 255).astype(np.uint8) * 255
        
        # åˆ›å»ºæ’•è£‚é¢åŒºåŸŸï¼šåŸå›¾ - å‰ªåˆ‡é¢åŒºåŸŸ
        # å°†å‰ªåˆ‡é¢åŒºåŸŸè®¾ä¸ºé»‘è‰²ï¼ˆ0ï¼‰ï¼Œä¿ç•™æ’•è£‚é¢åŒºåŸŸ
        tear_region = roi_image.copy()
        tear_region[shear_mask > 0] = 0  # å°†å‰ªåˆ‡é¢åŒºåŸŸè®¾ä¸ºé»‘è‰²
        
        return tear_region, shear_mask
    
    def create_tear_patch_visualization(self, original_image, filtered_tear_region, spot_result, output_dir, frame_num):
        """
        åˆ›å»ºæ’•è£‚é¢æ–‘å—å¯è§†åŒ–å›¾
        
        Args:
            original_image: åŸå§‹ROIå›¾åƒ
            filtered_tear_region: è¿‡æ»¤åçš„æ’•è£‚é¢åŒºåŸŸ
            spot_result: æ–‘å—æ£€æµ‹ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
            frame_num: å¸§å·
        """
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        font_success = setup_chinese_font()
        
        # åˆ›å»º2x2å­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'æ’•è£‚é¢æ–‘å—å¯è§†åŒ– - Frame {frame_num:06d}', fontsize=16)
        
        # å­å›¾1: åŸå§‹ROIå›¾åƒ
        axes[0, 0].imshow(original_image, cmap='gray')
        axes[0, 0].set_title('åŸå§‹ROIå›¾åƒ' if font_success else 'Original ROI Image')
        axes[0, 0].axis('off')
        
        # å­å›¾2: è¿‡æ»¤åçš„æ’•è£‚é¢åŒºåŸŸ
        axes[0, 1].imshow(filtered_tear_region, cmap='gray')
        axes[0, 1].set_title('è¿‡æ»¤åçš„æ’•è£‚é¢åŒºåŸŸ' if font_success else 'Filtered Tear Region')
        axes[0, 1].axis('off')
        
        # å­å›¾3: æ–‘å—æ£€æµ‹ç»“æœ
        if spot_result.get('success', False):
            # è¯»å–ç”Ÿæˆçš„æ–‘å—å›¾
            patch_image_path = os.path.join(output_dir, f"tear_patches_frame_{frame_num:06d}.png")
            if os.path.exists(patch_image_path):
                patch_image = cv2.imread(patch_image_path, cv2.IMREAD_COLOR)
                if patch_image is not None:
                    patch_image_rgb = cv2.cvtColor(patch_image, cv2.COLOR_BGR2RGB)
                    axes[1, 0].imshow(patch_image_rgb)
                    axes[1, 0].set_title('æ–‘å—æ£€æµ‹ç»“æœ' if font_success else 'Patch Detection Result')
                else:
                    axes[1, 0].imshow(filtered_tear_region, cmap='gray')
                    axes[1, 0].set_title('æ–‘å—å›¾è¯»å–å¤±è´¥' if font_success else 'Failed to Read Patch Image')
            else:
                axes[1, 0].imshow(filtered_tear_region, cmap='gray')
                axes[1, 0].set_title('æ–‘å—å›¾ä¸å­˜åœ¨' if font_success else 'Patch Image Not Found')
        else:
            axes[1, 0].imshow(filtered_tear_region, cmap='gray')
            axes[1, 0].set_title('æ–‘å—æ£€æµ‹å¤±è´¥' if font_success else 'Patch Detection Failed')
        axes[1, 0].axis('off')
        
        # å­å›¾4: å åŠ å¯è§†åŒ–
        # å°†åŸå§‹å›¾åƒè½¬æ¢ä¸ºRGB
        original_rgb = cv2.cvtColor(original_image, cv2.COLOR_GRAY2RGB)
        
        # åˆ›å»ºå åŠ å›¾åƒ
        overlay = original_rgb.copy()
        
        # åœ¨è¿‡æ»¤åçš„æ’•è£‚é¢åŒºåŸŸä¸Šå åŠ çº¢è‰²
        tear_mask = filtered_tear_region > 0
        overlay[tear_mask] = [255, 0, 0]  # çº¢è‰²è¡¨ç¤ºæ’•è£‚é¢åŒºåŸŸ
        
        # å¦‚æœæœ‰æ–‘å—æ£€æµ‹ç»“æœï¼Œåœ¨æ–‘å—ä¸Šå åŠ ç»¿è‰²
        if spot_result.get('success', False):
            # è¯»å–ç”Ÿæˆçš„æ–‘å—å›¾æ¥è·å–æ–‘å—ä½ç½®
            patch_image_path = os.path.join(output_dir, f"tear_patches_frame_{frame_num:06d}.png")
            if os.path.exists(patch_image_path):
                patch_image = cv2.imread(patch_image_path, cv2.IMREAD_COLOR)
                if patch_image is not None:
                    # å°†æ–‘å—å›¾è½¬æ¢ä¸ºç°åº¦å›¾ï¼Œæ‰¾åˆ°çº¢è‰²åŒºåŸŸï¼ˆæ–‘å—ï¼‰
                    patch_gray = cv2.cvtColor(patch_image, cv2.COLOR_BGR2GRAY)
                    # çº¢è‰²é€šé“é€šå¸¸å¯¹åº”æ–‘å—åŒºåŸŸ
                    spot_mask = patch_gray > 100  # é˜ˆå€¼å¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                    overlay[spot_mask] = [0, 255, 0]  # ç»¿è‰²è¡¨ç¤ºæ£€æµ‹åˆ°çš„æ–‘å—
        
        # æ··åˆæ˜¾ç¤º
        alpha = 0.6
        blended = cv2.addWeighted(original_rgb, 1-alpha, overlay, alpha, 0)
        
        axes[1, 1].imshow(blended)
        axes[1, 1].set_title('å åŠ å¯è§†åŒ–\n(çº¢: æ’•è£‚é¢, ç»¿: æ–‘å—)' if font_success else 'Overlay Visualization\n(Red: Tear, Green: Patches)')
        axes[1, 1].axis('off')
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if spot_result.get('success', False):
            patch_count = spot_result.get('spot_count', 0)
            patch_density = spot_result.get('spot_density', 0.0)
        else:
            patch_count = 0
            patch_density = 0.0
        
        stats_text = f"""
ç»Ÿè®¡ä¿¡æ¯:
æ–‘å—æ•°é‡: {patch_count}
æ–‘å—å¯†åº¦: {patch_density:.6f}
æ’•è£‚é¢åƒç´ : {np.sum(tear_mask)}
        """
        
        fig.text(0.02, 0.02, stats_text, fontsize=10, 
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8),
                verticalalignment='bottom')
        
        plt.tight_layout()
        plt.subplots_adjust(bottom=0.15)
        
        # ä¿å­˜å¯è§†åŒ–å›¾
        viz_filename = f"tear_patch_visualization_frame_{frame_num:06d}.png"
        viz_path = os.path.join(output_dir, viz_filename)
        plt.savefig(viz_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def analyze_roi_spots_with_tear_filter(self, roi_dir: str, output_dir: str = "output") -> List[Dict[str, Any]]:
        """
        åˆ†æROIå›¾åƒçš„æ’•è£‚é¢æ–‘å—ç‰¹å¾ï¼ˆä½¿ç”¨å‰ªåˆ‡é¢è¿‡æ»¤ï¼‰
        
        Args:
            roi_dir: ROIå›¾åƒç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            æ–‘å—åˆ†æç»“æœåˆ—è¡¨
        """
        print("å¼€å§‹åˆ†æROIå›¾åƒçš„æ’•è£‚é¢æ–‘å—ç‰¹å¾ï¼ˆä½¿ç”¨å‰ªåˆ‡é¢è¿‡æ»¤ï¼‰...")
        
        if self.skip_step1:
            print("âš ï¸  è·³è¿‡ç¬¬ä¸€æ­¥å¯è§†åŒ–ä¿å­˜ï¼ˆä»ä¼šè¿›è¡Œæ£€æµ‹ä»¥æ„å»ºç¼“å­˜ï¼‰")
        
        if self.viz_interval is not None:
            print(f"å¯è§†åŒ–ä¿å­˜é—´éš”: æ¯ {self.viz_interval} å¸§ä¿å­˜ä¸€æ¬¡")
        else:
            print("å¯è§†åŒ–ä¿å­˜é—´éš”: ä¿å­˜æ‰€æœ‰å¸§")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»ºå„æ­¥éª¤ä¿å­˜ç›®å½•
        step1_dir = os.path.join(output_dir, 'step1_shear_tear_masks')
        step2_dir = os.path.join(output_dir, 'step2_filtered_tear_regions')
        step3_dir = os.path.join(output_dir, 'step3_tear_patch_analysis')
        
        # åªæœ‰ä¸è·³è¿‡ç¬¬ä¸€æ­¥æ—¶æ‰åˆ›å»ºstep1ç›®å½•
        if not self.skip_step1:
            os.makedirs(step1_dir, exist_ok=True)
        os.makedirs(step2_dir, exist_ok=True)
        os.makedirs(step3_dir, exist_ok=True)
        
        # è·å–æ‰€æœ‰ROIå›¾åƒæ–‡ä»¶
        roi_pattern = os.path.join(roi_dir, "*_roi.png")
        roi_files = sorted(glob.glob(roi_pattern), key=self.extract_frame_info)
        
        if not roi_files:
            print(f"åœ¨ç›®å½• {roi_dir} ä¸­æœªæ‰¾åˆ°ROIå›¾åƒæ–‡ä»¶")
            return []
        
        print(f"æ‰¾åˆ° {len(roi_files)} ä¸ªROIå›¾åƒæ–‡ä»¶")
        
        results = []
        
        # ç¼“å­˜ä¸­é—´ç»“æœ
        segmentation_cache = {}  # å­˜å‚¨æ¯å¸§çš„åˆ†å‰²ç»“æœ
        roi_image_cache = {}  # å­˜å‚¨æ¯å¸§çš„ROIå›¾åƒ
        
        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå‰ªåˆ‡é¢å’Œæ’•è£‚é¢mask
        if self.skip_step1:
            print("\nç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå‰ªåˆ‡é¢å’Œæ’•è£‚é¢maskï¼ˆè·³è¿‡å¯è§†åŒ–ä¿å­˜ï¼‰...")
        else:
            print("\nç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå‰ªåˆ‡é¢å’Œæ’•è£‚é¢mask...")
        
        saved_count = 0
        for idx, roi_file in enumerate(tqdm(roi_files, desc="ç”Ÿæˆå‰ªåˆ‡é¢æ’•è£‚é¢mask", unit="å›¾åƒ")):
            frame_num = self.extract_frame_info(roi_file)
            if frame_num == -1:
                continue
                
            try:
                # è¯»å–ROIå›¾åƒ
                roi_image = cv2.imread(roi_file, cv2.IMREAD_GRAYSCALE)
                if roi_image is None:
                    continue
                
                # ä½¿ç”¨ShearTearDetectoræ£€æµ‹å‰ªåˆ‡é¢å’Œæ’•è£‚é¢ï¼ˆæ‰€æœ‰å¸§éƒ½è¦è®¡ç®—ï¼‰
                result = self.shear_tear_detector.detect_surfaces(roi_image, visualize=False)
                if result and 'segmented_image' in result:
                    segmented_image = result['segmented_image']
                    
                    # ç¡®ä¿åˆ†å‰²å›¾åƒå’ŒåŸå›¾å°ºå¯¸ä¸€è‡´
                    if roi_image.shape != segmented_image.shape:
                        # å¦‚æœå°ºå¯¸ä¸ä¸€è‡´ï¼Œå°†åˆ†å‰²å›¾åƒresizeåˆ°ä¸åŸå›¾ç›¸åŒçš„å°ºå¯¸
                        segmented_image = cv2.resize(segmented_image, 
                                                    (roi_image.shape[1], roi_image.shape[0]), 
                                                    interpolation=cv2.INTER_NEAREST)
                    
                    # ç¼“å­˜ç»“æœä¾›åç»­æ­¥éª¤ä½¿ç”¨
                    segmentation_cache[frame_num] = segmented_image
                    roi_image_cache[frame_num] = roi_image
                    
                    # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿å­˜å¯è§†åŒ–ï¼ˆå¦‚æœè·³è¿‡ç¬¬ä¸€æ­¥ï¼Œåˆ™ä¸ä¿å­˜ï¼‰
                    if not self.skip_step1:
                        should_save = (self.viz_interval is None) or (idx % self.viz_interval == 0)
                        
                        if should_save:
                            # ä¿å­˜å‰ªåˆ‡é¢mask
                            shear_mask = (segmented_image == 255).astype(np.uint8) * 255
                            shear_filename = f"shear_mask_frame_{frame_num:06d}.png"
                            shear_path = os.path.join(step1_dir, shear_filename)
                            cv2.imwrite(shear_path, shear_mask)
                            
                            # ä¿å­˜æ’•è£‚é¢mask
                            tear_mask = (segmented_image == 128).astype(np.uint8) * 255
                            tear_filename = f"tear_mask_frame_{frame_num:06d}.png"
                            tear_path = os.path.join(step1_dir, tear_filename)
                            cv2.imwrite(tear_path, tear_mask)
                            
                            saved_count += 1
                    
            except Exception as e:
                print(f"ç”Ÿæˆmaskæ—¶å‡ºé”™ {roi_file}: {e}")
                continue
        
        if self.skip_step1:
            print(f"ç¬¬ä¸€æ­¥å®Œæˆ: è®¡ç®—äº† {len(segmentation_cache)} å¸§ï¼ˆå·²è·³è¿‡å¯è§†åŒ–ä¿å­˜ï¼‰")
        else:
            print(f"ç¬¬ä¸€æ­¥å®Œæˆ: è®¡ç®—äº† {len(segmentation_cache)} å¸§ï¼Œä¿å­˜äº† {saved_count} å¸§å¯è§†åŒ–åˆ°: {step1_dir}")
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨å‰ªåˆ‡é¢maskè¿‡æ»¤å‡ºæ’•è£‚é¢åŒºåŸŸ
        print("\nç¬¬äºŒæ­¥ï¼šè¿‡æ»¤æ’•è£‚é¢åŒºåŸŸ...")
        saved_count = 0
        for idx, roi_file in enumerate(tqdm(roi_files, desc="è¿‡æ»¤æ’•è£‚é¢åŒºåŸŸ", unit="å›¾åƒ")):
            frame_num = self.extract_frame_info(roi_file)
            if frame_num == -1:
                continue
                
            try:
                # ä»ç¼“å­˜ä¸­è·å–åˆ†å‰²ç»“æœå’ŒROIå›¾åƒ
                if frame_num not in segmentation_cache or frame_num not in roi_image_cache:
                    continue
                
                segmented_image = segmentation_cache[frame_num]
                roi_image = roi_image_cache[frame_num]
                
                # ä½¿ç”¨å‰ªåˆ‡é¢maskè¿‡æ»¤å‡ºæ’•è£‚é¢åŒºåŸŸï¼ˆæ‰€æœ‰å¸§éƒ½è¦è®¡ç®—ï¼‰
                filtered_tear_region, shear_mask = self.filter_tear_region_with_shear_mask(roi_image, segmented_image)
                
                # ã€ä¼˜åŒ–ã€‘ç›´æ¥è°ƒç”¨ feature_extractor è¿›è¡Œæ–‘å—æ£€æµ‹ï¼ˆæ‰€æœ‰å¸§éƒ½è¦è®¡ç®—ï¼‰
                # ä¸ç”Ÿæˆå¯è§†åŒ–å›¾ï¼Œå¤§å¹…æé€Ÿ
                spot_result = self.feature_extractor.detect_all_white_spots(roi_image)
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿å­˜å¯è§†åŒ–
                should_save = (self.viz_interval is None) or (idx % self.viz_interval == 0)
                
                if should_save:
                    # ä¿å­˜è¿‡æ»¤åçš„æ’•è£‚é¢åŒºåŸŸ
                    filtered_filename = f"filtered_tear_region_frame_{frame_num:06d}.png"
                    filtered_path = os.path.join(step2_dir, filtered_filename)
                    cv2.imwrite(filtered_path, filtered_tear_region)
                    
                    # ç”Ÿæˆå¹¶ä¿å­˜æ–‘å—å›¾
                    patch_filename = f"tear_patches_frame_{frame_num:06d}.png"
                    patch_path = os.path.join(step2_dir, patch_filename)
                    spot_binary = spot_result.get('all_white_binary_mask', None)
                    if spot_binary is not None:
                        spot_visualization = self.spot_processor.create_spot_visualization(roi_image, spot_binary)
                        cv2.imwrite(patch_path, spot_visualization)
                    
                    # åˆ›å»ºæ’•è£‚é¢æ–‘å—å¯è§†åŒ–å›¾
                    spot_result_with_path = {
                        'success': True,
                        'spot_count': spot_result.get('all_spot_count', 0),
                        'spot_density': spot_result.get('all_spot_density', 0.0),
                        'output_path': patch_path
                    }
                    self.create_tear_patch_visualization(roi_image, filtered_tear_region, spot_result_with_path, step2_dir, frame_num)
                    
                    saved_count += 1
                
                # æå–å…³é”®ä¿¡æ¯ï¼ˆæ‰€æœ‰å¸§éƒ½è¦è®°å½•ï¼‰
                result_data = {
                    'frame_num': frame_num,
                    'time_seconds': frame_num * 5,  # å‡è®¾æ¯5ç§’ä¸€å¸§
                    'spot_count': spot_result.get('all_spot_count', 0),
                    'spot_density': spot_result.get('all_spot_density', 0.0),
                    'image_shape': roi_image.shape,
                    'roi_file': roi_file
                }
                
                results.append(result_data)
                    
            except Exception as e:
                print(f"åˆ†æROIå›¾åƒ {roi_file} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"ç¬¬äºŒæ­¥å®Œæˆ: è®¡ç®—äº† {len(results)} å¸§ï¼Œä¿å­˜äº† {saved_count} å¸§å¯è§†åŒ–åˆ°: {step2_dir}")
        
        # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæ—¶é—´åºåˆ—åˆ†æ
        print("\nç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæ—¶é—´åºåˆ—åˆ†æ...")
        if results:
            # åˆ›å»ºæ—¶é—´åºåˆ—å›¾è¡¨
            plot_path = self.create_temporal_plots(results, step3_dir)
            
            # ä¿å­˜æ•°æ®
            self.save_data_to_csv(results, step3_dir)
            
            # è¾“å‡ºç»Ÿè®¡æ‘˜è¦
            self.print_statistics_summary(results)
            
            print(f"ç¬¬ä¸‰æ­¥å®Œæˆï¼Œåˆ†æç»“æœå’Œæ›²çº¿å›¾å·²ä¿å­˜åˆ°: {step3_dir}")
        
        print(f"æ–‘å—åˆ†æå®Œæˆï¼ŒæˆåŠŸåˆ†æ {len(results)} ä¸ªROIå›¾åƒ")
        return results
    
    def apply_smoothing_filters(self, data: List[Dict[str, Any]], 
                               smoothing_method: str = 'gaussian',
                               window_size: int = 50,
                               sigma: float = 10.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        å¯¹æ—¶é—´åºåˆ—æ•°æ®åº”ç”¨å¹³æ»‘æ»¤æ³¢
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            smoothing_method: å¹³æ»‘æ–¹æ³• ('gaussian', 'moving_avg', 'savgol', 'median')
            window_size: æ»¤æ³¢çª—å£å¤§å°
            sigma: é«˜æ–¯æ»¤æ³¢çš„æ ‡å‡†å·®
            
        Returns:
            æ—¶é—´åºåˆ—ã€å¹³æ»‘åçš„æ–‘å—æ•°é‡ã€å¹³æ»‘åçš„æ–‘å—å¯†åº¦
        """
        time_seconds = np.array([d['time_seconds'] for d in data])
        spot_counts = np.array([d['spot_count'] for d in data])
        spot_densities = np.array([d['spot_density'] for d in data])
        
        if smoothing_method == 'gaussian':
            # é«˜æ–¯æ»¤æ³¢
            smoothed_counts = gaussian_filter1d(spot_counts, sigma=sigma)
            smoothed_densities = gaussian_filter1d(spot_densities, sigma=sigma)
            
        elif smoothing_method == 'moving_avg':
            # ç§»åŠ¨å¹³å‡æ»¤æ³¢
            smoothed_counts = np.convolve(spot_counts, np.ones(window_size)/window_size, mode='same')
            smoothed_densities = np.convolve(spot_densities, np.ones(window_size)/window_size, mode='same')
            
        elif smoothing_method == 'savgol':
            # Savitzky-Golayæ»¤æ³¢
            window_length = min(window_size, len(spot_counts))
            if window_length % 2 == 0:
                window_length -= 1
            smoothed_counts = signal.savgol_filter(spot_counts, window_length, 3)
            smoothed_densities = signal.savgol_filter(spot_densities, window_length, 3)
            
        elif smoothing_method == 'median':
            # ä¸­å€¼æ»¤æ³¢
            smoothed_counts = signal.medfilt(spot_counts, kernel_size=window_size)
            smoothed_densities = signal.medfilt(spot_densities, kernel_size=window_size)
            
        else:
            # é»˜è®¤ä½¿ç”¨é«˜æ–¯æ»¤æ³¢
            smoothed_counts = gaussian_filter1d(spot_counts, sigma=sigma)
            smoothed_densities = gaussian_filter1d(spot_densities, sigma=sigma)
        
        return time_seconds, smoothed_counts, smoothed_densities
    
    def create_temporal_plots(self, data: List[Dict[str, Any]], output_dir: str = "output",
                            smoothing_method: str = 'gaussian', window_size: int = 50, sigma: float = 10.0):
        """
        åˆ›å»ºæ—¶é—´åºåˆ—å›¾è¡¨ï¼ˆå¸¦å¹³æ»‘æ»¤æ³¢ï¼‰
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            output_dir: è¾“å‡ºç›®å½•
            smoothing_method: å¹³æ»‘æ–¹æ³•
            window_size: æ»¤æ³¢çª—å£å¤§å°
            sigma: é«˜æ–¯æ»¤æ³¢æ ‡å‡†å·®
        """
        if not data:
            print("æ²¡æœ‰æ•°æ®å¯ä»¥ç»˜åˆ¶")
            return
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        font_success = setup_chinese_font()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # æå–åŸå§‹æ•°æ®
        time_seconds = np.array([d['time_seconds'] for d in data])
        spot_counts = np.array([d['spot_count'] for d in data])
        spot_densities = np.array([d['spot_density'] for d in data])
        
        # åº”ç”¨å¹³æ»‘æ»¤æ³¢
        _, smoothed_counts, smoothed_densities = self.apply_smoothing_filters(
            data, smoothing_method, window_size, sigma)
        
        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))
        
        # æ’•è£‚é¢æ–‘å—æ•°é‡éšæ—¶é—´å˜åŒ–ï¼ˆåŸå§‹æ•°æ®+å¹³æ»‘æ›²çº¿ï¼‰
        ax1.plot(time_seconds, spot_counts, 'b-', linewidth=0.8, alpha=0.3, label='åŸå§‹æ•°æ®')
        ax1.plot(time_seconds, smoothed_counts, 'b-', linewidth=2.5, alpha=0.9, label='å¹³æ»‘æ›²çº¿')
        ax1.fill_between(time_seconds, smoothed_counts, alpha=0.3, color='blue')
        ax1.set_xlabel('æ—¶é—´ (ç§’)' if font_success else 'Time (seconds)')
        ax1.set_ylabel('æ’•è£‚é¢æ–‘å—æ•°é‡' if font_success else 'Tear Spot Count')
        ax1.set_title('æ’•è£‚é¢æ–‘å—æ•°é‡éšæ—¶é—´å˜åŒ– (å¹³æ»‘æ»¤æ³¢)' if font_success else 'Tear Spot Count Over Time (Smoothed)')
        ax1.grid(True, alpha=0.3)
        ax1.set_xlim(0, max(time_seconds))
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_count = np.mean(spot_counts)
        max_count = np.max(smoothed_counts)
        ax1.axhline(y=mean_count, color='red', linestyle='--', alpha=0.7, 
                   label=f'å¹³å‡å€¼: {mean_count:.1f}')
        ax1.legend()
        
        # æ’•è£‚é¢æ–‘å—å¯†åº¦éšæ—¶é—´å˜åŒ–ï¼ˆåŸå§‹æ•°æ®+å¹³æ»‘æ›²çº¿ï¼‰
        ax2.plot(time_seconds, spot_densities, 'r-', linewidth=0.8, alpha=0.3, label='åŸå§‹æ•°æ®')
        ax2.plot(time_seconds, smoothed_densities, 'r-', linewidth=2.5, alpha=0.9, label='å¹³æ»‘æ›²çº¿')
        ax2.fill_between(time_seconds, smoothed_densities, alpha=0.3, color='red')
        ax2.set_xlabel('æ—¶é—´ (ç§’)' if font_success else 'Time (seconds)')
        ax2.set_ylabel('æ’•è£‚é¢æ–‘å—å¯†åº¦' if font_success else 'Tear Spot Density')
        ax2.set_title('æ’•è£‚é¢æ–‘å—å¯†åº¦éšæ—¶é—´å˜åŒ– (å¹³æ»‘æ»¤æ³¢)' if font_success else 'Tear Spot Density Over Time (Smoothed)')
        ax2.grid(True, alpha=0.3)
        ax2.set_xlim(0, max(time_seconds))
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_density = np.mean(spot_densities)
        max_density = np.max(smoothed_densities)
        ax2.axhline(y=mean_density, color='blue', linestyle='--', alpha=0.7,
                   label=f'å¹³å‡å€¼: {mean_density:.4f}')
        ax2.legend()
        
        # æ·»åŠ æ»¤æ³¢æ–¹æ³•è¯´æ˜
        method_names = {
            'gaussian': 'é«˜æ–¯æ»¤æ³¢',
            'moving_avg': 'ç§»åŠ¨å¹³å‡',
            'savgol': 'Savitzky-Golayæ»¤æ³¢',
            'median': 'ä¸­å€¼æ»¤æ³¢'
        }
        method_name = method_names.get(smoothing_method, smoothing_method)
        fig.suptitle(f'å¹³æ»‘æ–¹æ³•: {method_name} (Ïƒ={sigma}, çª—å£={window_size})', fontsize=12, y=0.98)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.95)
        
        # ä¿å­˜å›¾è¡¨
        plot_path = os.path.join(output_dir, "tear_spot_temporal_analysis_smoothed.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"å¹³æ»‘æ—¶é—´åºåˆ—å›¾è¡¨å·²ä¿å­˜: {plot_path}")
        
        # åˆ›å»ºç»Ÿè®¡æ‘˜è¦å›¾
        self.create_statistics_summary(data, output_dir)
        
        return plot_path
    
    def create_statistics_summary(self, data: List[Dict[str, Any]], output_dir: str):
        """
        åˆ›å»ºç»Ÿè®¡æ‘˜è¦å›¾è¡¨
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        if not data:
            return
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        font_success = setup_chinese_font()
        
        # æå–æ•°æ®
        spot_counts = [d['spot_count'] for d in data]
        spot_densities = [d['spot_density'] for d in data]
        
        # åˆ›å»ºç»Ÿè®¡å›¾è¡¨
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # æ’•è£‚é¢æ–‘å—æ•°é‡åˆ†å¸ƒç›´æ–¹å›¾
        ax1.hist(spot_counts, bins=30, alpha=0.7, color='blue', edgecolor='black')
        ax1.set_xlabel('æ’•è£‚é¢æ–‘å—æ•°é‡' if font_success else 'Tear Spot Count')
        ax1.set_ylabel('é¢‘æ¬¡' if font_success else 'Frequency')
        ax1.set_title('æ’•è£‚é¢æ–‘å—æ•°é‡åˆ†å¸ƒ' if font_success else 'Tear Spot Count Distribution')
        ax1.grid(True, alpha=0.3)
        
        # æ’•è£‚é¢æ–‘å—å¯†åº¦åˆ†å¸ƒç›´æ–¹å›¾
        ax2.hist(spot_densities, bins=30, alpha=0.7, color='red', edgecolor='black')
        ax2.set_xlabel('æ’•è£‚é¢æ–‘å—å¯†åº¦' if font_success else 'Tear Spot Density')
        ax2.set_ylabel('é¢‘æ¬¡' if font_success else 'Frequency')
        ax2.set_title('æ’•è£‚é¢æ–‘å—å¯†åº¦åˆ†å¸ƒ' if font_success else 'Tear Spot Density Distribution')
        ax2.grid(True, alpha=0.3)
        
        # æ’•è£‚é¢æ–‘å—æ•°é‡ä¸å¯†åº¦çš„æ•£ç‚¹å›¾
        ax3.scatter(spot_counts, spot_densities, alpha=0.6, color='green')
        ax3.set_xlabel('æ’•è£‚é¢æ–‘å—æ•°é‡' if font_success else 'Tear Spot Count')
        ax3.set_ylabel('æ’•è£‚é¢æ–‘å—å¯†åº¦' if font_success else 'Tear Spot Density')
        ax3.set_title('æ’•è£‚é¢æ–‘å—æ•°é‡ä¸å¯†åº¦å…³ç³»' if font_success else 'Tear Spot Count vs Density')
        ax3.grid(True, alpha=0.3)
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = np.corrcoef(spot_counts, spot_densities)[0, 1]
        ax3.text(0.05, 0.95, f'ç›¸å…³ç³»æ•°: {correlation:.3f}', 
                transform=ax3.transAxes, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        # ç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
        ax4.axis('off')
        stats_text = f"""
ç»Ÿè®¡æ‘˜è¦:

æ’•è£‚é¢æ–‘å—æ•°é‡:
  å¹³å‡å€¼: {np.mean(spot_counts):.2f}
  æ ‡å‡†å·®: {np.std(spot_counts):.2f}
  æœ€å°å€¼: {np.min(spot_counts)}
  æœ€å¤§å€¼: {np.max(spot_counts)}
  ä¸­ä½æ•°: {np.median(spot_counts):.2f}

æ’•è£‚é¢æ–‘å—å¯†åº¦:
  å¹³å‡å€¼: {np.mean(spot_densities):.6f}
  æ ‡å‡†å·®: {np.std(spot_densities):.6f}
  æœ€å°å€¼: {np.min(spot_densities):.6f}
  æœ€å¤§å€¼: {np.max(spot_densities):.6f}
  ä¸­ä½æ•°: {np.median(spot_densities):.6f}

æ•°æ®ç‚¹æ€»æ•°: {len(data)}
        """
        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜ç»Ÿè®¡å›¾è¡¨
        stats_path = os.path.join(output_dir, "tear_spot_statistics_summary.png")
        plt.savefig(stats_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ç»Ÿè®¡æ‘˜è¦å›¾è¡¨å·²ä¿å­˜: {stats_path}")
    
    def save_data_to_csv(self, data: List[Dict[str, Any]], output_dir: str = "output"):
        """
        ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        if not data:
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(data)
        
        # ä¿å­˜CSVæ–‡ä»¶
        csv_path = os.path.join(output_dir, "tear_spot_temporal_data.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8')
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {csv_path}")
        
        # ä¿å­˜JSONæ ¼å¼
        json_path = os.path.join(output_dir, "tear_spot_temporal_data.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ°JSONæ–‡ä»¶: {json_path}")
    
    def run_analysis(self, roi_dir: str = "data/roi_imgs", output_dir: str = "output",
                    smoothing_method: str = 'gaussian', window_size: int = 50, sigma: float = 10.0):
        """
        è¿è¡Œå®Œæ•´çš„æ’•è£‚é¢æ–‘å—æ—¶é—´åºåˆ—åˆ†æï¼ˆä½¿ç”¨å‰ªåˆ‡é¢è¿‡æ»¤ï¼‰
        
        Args:
            roi_dir: ROIå›¾åƒç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            smoothing_method: å¹³æ»‘æ–¹æ³• ('gaussian', 'moving_avg', 'savgol', 'median')
            window_size: æ»¤æ³¢çª—å£å¤§å°
            sigma: é«˜æ–¯æ»¤æ³¢æ ‡å‡†å·®
        """
        print("=== æ’•è£‚é¢æ–‘å—æ—¶é—´åºåˆ—åˆ†æ (ä½¿ç”¨å‰ªåˆ‡é¢è¿‡æ»¤) ===")
        
        # åˆ†æROIå›¾åƒçš„æ’•è£‚é¢æ–‘å—ç‰¹å¾
        data = self.analyze_roi_spots_with_tear_filter(roi_dir, output_dir)
        
        if not data:
            print("æ²¡æœ‰å¯åˆ†æçš„æ•°æ®")
            return
        
        print(f"\nâœ… æ’•è£‚é¢æ–‘å—æ—¶é—´åºåˆ—åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š åˆ†æç»“æœä¿å­˜ä½ç½®: {output_dir}")
        print(f"ğŸ”§ å¹³æ»‘æ–¹æ³•: {smoothing_method}, çª—å£å¤§å°: {window_size}, Ïƒ: {sigma}")
        
        return data
    
    def print_statistics_summary(self, data: List[Dict[str, Any]]):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        if not data:
            return
        
        spot_counts = [d['spot_count'] for d in data]
        spot_densities = [d['spot_density'] for d in data]
        
        print("\n=== æ’•è£‚é¢æ–‘å—æ—¶é—´åºåˆ—ç»Ÿè®¡æ‘˜è¦ ===")
        print(f"æ•°æ®ç‚¹æ€»æ•°: {len(data)}")
        print(f"æ—¶é—´è·¨åº¦: {data[0]['time_seconds']:.1f} - {data[-1]['time_seconds']:.1f} ç§’")
        print(f"å¸§æ•°èŒƒå›´: {data[0]['frame_num']} - {data[-1]['frame_num']}")
        
        print("\næ’•è£‚é¢æ–‘å—æ•°é‡ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {np.mean(spot_counts):.2f}")
        print(f"  æ ‡å‡†å·®: {np.std(spot_counts):.2f}")
        print(f"  æœ€å°å€¼: {np.min(spot_counts)}")
        print(f"  æœ€å¤§å€¼: {np.max(spot_counts)}")
        print(f"  ä¸­ä½æ•°: {np.median(spot_counts):.2f}")
        
        print("\næ’•è£‚é¢æ–‘å—å¯†åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {np.mean(spot_densities):.6f}")
        print(f"  æ ‡å‡†å·®: {np.std(spot_densities):.6f}")
        print(f"  æœ€å°å€¼: {np.min(spot_densities):.6f}")
        print(f"  æœ€å¤§å€¼: {np.max(spot_densities):.6f}")
        print(f"  ä¸­ä½æ•°: {np.median(spot_densities):.6f}")
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = np.corrcoef(spot_counts, spot_densities)[0, 1]
        print(f"\næ’•è£‚é¢æ–‘å—æ•°é‡ä¸å¯†åº¦ç›¸å…³ç³»æ•°: {correlation:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ’•è£‚é¢æ–‘å—æ—¶é—´åºåˆ—åˆ†æï¼ˆä½¿ç”¨å‰ªåˆ‡é¢maskè¿‡æ»¤ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨
  python analyze_split_temporal_filter_tear.py --roi_dir data/roi_imgs --output_dir output/tear_filter_analysis
  
  # æŒ‡å®šå¹³æ»‘å‚æ•°å’Œå¯è§†åŒ–é—´éš”
  python analyze_split_temporal_filter_tear.py --roi_dir data/roi_imgs --output_dir output/tear_filter_analysis \\
      --smoothing_method gaussian --sigma 10.0 --window_size 50 --viz_interval 100
  
  # è·³è¿‡ç¬¬ä¸€æ­¥çš„å¯è§†åŒ–ä¿å­˜ï¼ˆç¬¬ä¸€æ­¥å·²è¿è¡Œè¿‡ï¼‰
  python analyze_split_temporal_filter_tear.py --roi_dir data/roi_imgs --output_dir output/tear_filter_analysis \\
      --skip_step1
  
  # è·³è¿‡ç¬¬ä¸€æ­¥+è‡ªå®šä¹‰å¯è§†åŒ–é—´éš”
  python analyze_split_temporal_filter_tear.py --roi_dir data/roi_imgs --output_dir output/tear_filter_analysis \\
      --skip_step1 --viz_interval 200
  
  # ä¸ä¿å­˜ä»»ä½•å¯è§†åŒ–ï¼ˆä»…ç”Ÿæˆæ•°æ®å’Œæ›²çº¿ï¼‰
  python analyze_split_temporal_filter_tear.py --roi_dir data/roi_imgs --output_dir output/tear_filter_analysis \\
      --viz_interval 0
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--roi_dir', type=str, required=True,
                        help='ROIå›¾åƒç›®å½•è·¯å¾„')
    parser.add_argument('--output_dir', type=str, required=True,
                        help='è¾“å‡ºç›®å½•è·¯å¾„')
    
    # å¹³æ»‘å‚æ•°
    parser.add_argument('--smoothing_method', type=str, 
                        choices=['gaussian', 'moving_avg', 'savgol', 'median'],
                        default='gaussian',
                        help='å¹³æ»‘æ–¹æ³• (é»˜è®¤: gaussian)')
    parser.add_argument('--window_size', type=int, default=50,
                        help='æ»¤æ³¢çª—å£å¤§å° (é»˜è®¤: 50)')
    parser.add_argument('--sigma', type=float, default=10.0,
                        help='é«˜æ–¯æ»¤æ³¢æ ‡å‡†å·® (é»˜è®¤: 10.0)')
    
    # å¯è§†åŒ–æ§åˆ¶å‚æ•°
    parser.add_argument('--viz_interval', type=int, default=100,
                        help='å¯è§†åŒ–ä¿å­˜é—´éš”ï¼ˆé»˜è®¤: 100ï¼›è®¾ç½®ä¸ºNoneä¿å­˜æ‰€æœ‰å¸§ï¼Œè®¾ç½®ä¸º0è¡¨ç¤ºä¸ä¿å­˜å¯è§†åŒ–ï¼‰')
    
    # æ­¥éª¤æ§åˆ¶å‚æ•°
    parser.add_argument('--skip_step1', action='store_true',
                        help='è·³è¿‡ç¬¬ä¸€æ­¥çš„å¯è§†åŒ–ä¿å­˜ï¼ˆä»ä¼šè¿›è¡Œæ£€æµ‹ä»¥æ„å»ºç¼“å­˜ï¼Œé€‚åˆç¬¬ä¸€æ­¥å·²è¿è¡Œè¿‡çš„æƒ…å†µï¼‰')
    
    args = parser.parse_args()
    
    # å¤„ç† viz_interval=0 çš„æƒ…å†µï¼ˆä¸ä¿å­˜ä»»ä½•å¯è§†åŒ–ï¼‰
    if args.viz_interval == 0:
        print("âš ï¸  viz_interval=0: å°†ä¸ä¿å­˜ä»»ä½•ä¸­é—´å¯è§†åŒ–ç»“æœ")
        viz_interval = 1  # è®¾ç½®ä¸€ä¸ªå¾ˆå¤§çš„å€¼æ¥è·³è¿‡æ‰€æœ‰å¯è§†åŒ–
        # å®é™…ä¸Šæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªç‰¹æ®Šæ ‡å¿—æ¥å®Œå…¨è·³è¿‡
    else:
        viz_interval = args.viz_interval
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = TearFilterTemporalAnalyzer(viz_interval=viz_interval, skip_step1=args.skip_step1)
    
    # è¿è¡Œåˆ†æ
    data = analyzer.run_analysis(
        roi_dir=args.roi_dir,
        output_dir=args.output_dir,
        smoothing_method=args.smoothing_method,
        window_size=args.window_size,
        sigma=args.sigma
    )
    
    if data:
        print(f"\nğŸ¯ åˆ†æå®Œæˆï¼å…±åˆ†æäº† {len(data)} ä¸ªæ—¶é—´ç‚¹çš„æ’•è£‚é¢æ–‘å—æ•°æ®")
        print("ğŸ“ˆ ç”Ÿæˆäº†å¹³æ»‘æ—¶é—´åºåˆ—æ›²çº¿å›¾å’Œç»Ÿè®¡æ‘˜è¦")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.output_dir}")


if __name__ == "__main__":
    main()
