#!/usr/bin/env python3
"""
æ’•è£‚é¢æ–‘å—æ•°é‡å’Œå¯†åº¦æ—¶é—´åºåˆ—åˆ†æï¼ˆä½¿ç”¨å‰ªåˆ‡é¢maskè¿‡æ»¤æ’•è£‚é¢ï¼‰
ä¸»è¦åŠŸèƒ½ï¼š
1. ä½¿ç”¨å‰ªåˆ‡é¢maskè¿‡æ»¤å‡ºæ’•è£‚é¢åŒºåŸŸ
2. ä»æ’•è£‚é¢åŒºåŸŸä¸­æå–æ–‘å—æ•°é‡å’Œå¯†åº¦æ•°æ®
3. ç»˜åˆ¶æ–‘å—æ•°é‡å’Œå¯†åº¦éšæ—¶é—´å˜åŒ–çš„æ›²çº¿å›¾
4. ç”Ÿæˆæ—¶é—´åºåˆ—åˆ†ææŠ¥å‘Š
"""

import cv2
import numpy as np
import os
import glob
import sys
import matplotlib.pyplot as plt
import pandas as pd
from typing import List, Dict, Any, Tuple
import json
import platform
from scipy import signal
from scipy.ndimage import gaussian_filter1d
from tqdm import tqdm

# æ·»åŠ data_processç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'data_process'))
# æ·»åŠ data_shear_splitç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'data_shear_split'))

from feature_extractor import FeatureExtractor
from config import PREPROCESS_CONFIG
from shear_tear_detector import ShearTearDetector

# è®¾ç½®ä¸­æ–‡å­—ä½“
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    import matplotlib.font_manager as fm
    
    system = platform.system()
    
    if system == "Darwin":  # macOS
        available_fonts = [f.name for f in fm.fontManager.ttflist]
        chinese_fonts = []
        
        preferred_fonts = ['PingFang SC', 'Hiragino Sans GB', 'STHeiti', 'Arial Unicode MS']
        
        for font in preferred_fonts:
            if font in available_fonts:
                chinese_fonts.append(font)
        
        if chinese_fonts:
            plt.rcParams['font.sans-serif'] = chinese_fonts + ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            print(f"è®¾ç½®ä¸­æ–‡å­—ä½“: {chinese_fonts[0]}")
            return True
    
    elif system == "Windows":
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        print("è®¾ç½®Windowsä¸­æ–‡å­—ä½“")
        return True
    
    else:  # Linux
        plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'Droid Sans Fallback', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        print("è®¾ç½®Linuxä¸­æ–‡å­—ä½“")
        return True
    
    print("æ— æ³•è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
    return False

class TearFilterTemporalAnalyzer:
    """æ’•è£‚é¢è¿‡æ»¤æ—¶é—´åºåˆ—åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.feature_extractor = FeatureExtractor(PREPROCESS_CONFIG)
        self.shear_tear_detector = ShearTearDetector()
        self.data = []
        
    def extract_frame_info(self, filename: str) -> int:
        """ä»æ–‡ä»¶åæå–å¸§å·"""
        try:
            basename = os.path.basename(filename)
            # æå–frame_XXXXXXä¸­çš„æ•°å­—
            frame_num = int(basename.split('_')[1])
            return frame_num
        except (IndexError, ValueError):
            return -1
    
    def filter_tear_region_with_shear_mask(self, roi_image, segmented_image):
        """
        ä½¿ç”¨å‰ªåˆ‡é¢maskè¿‡æ»¤å‡ºæ’•è£‚é¢åŒºåŸŸ
        
        Args:
            roi_image: åŸå§‹ROIå›¾åƒ
            segmented_image: åˆ†å‰²ç»“æœå›¾åƒï¼ˆ128=æ’•è£‚é¢ï¼Œ255=å‰ªåˆ‡é¢ï¼‰
            
        Returns:
            è¿‡æ»¤åçš„æ’•è£‚é¢åŒºåŸŸå›¾åƒ
        """
        # ä»åˆ†å‰²ç»“æœä¸­æå–å‰ªåˆ‡é¢maskï¼ˆå€¼ä¸º255çš„åŒºåŸŸï¼‰
        shear_mask = (segmented_image == 255).astype(np.uint8) * 255
        
        # åˆ›å»ºæ’•è£‚é¢åŒºåŸŸï¼šåŸå›¾ - å‰ªåˆ‡é¢åŒºåŸŸ
        # å°†å‰ªåˆ‡é¢åŒºåŸŸè®¾ä¸ºé»‘è‰²ï¼ˆ0ï¼‰ï¼Œä¿ç•™æ’•è£‚é¢åŒºåŸŸ
        tear_region = roi_image.copy()
        tear_region[shear_mask > 0] = 0  # å°†å‰ªåˆ‡é¢åŒºåŸŸè®¾ä¸ºé»‘è‰²
        
        return tear_region, shear_mask
    
    def analyze_roi_spots_with_tear_filter(self, roi_dir: str, output_dir: str = "output") -> List[Dict[str, Any]]:
        """
        åˆ†æROIå›¾åƒçš„æ’•è£‚é¢æ–‘å—ç‰¹å¾ï¼ˆä½¿ç”¨å‰ªåˆ‡é¢è¿‡æ»¤ï¼‰
        
        Args:
            roi_dir: ROIå›¾åƒç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            æ–‘å—åˆ†æç»“æœåˆ—è¡¨
        """
        print("å¼€å§‹åˆ†æROIå›¾åƒçš„æ’•è£‚é¢æ–‘å—ç‰¹å¾ï¼ˆä½¿ç”¨å‰ªåˆ‡é¢è¿‡æ»¤ï¼‰...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»ºå„æ­¥éª¤ä¿å­˜ç›®å½•
        step1_dir = os.path.join(output_dir, 'step1_shear_tear_masks')
        step2_dir = os.path.join(output_dir, 'step2_filtered_tear_regions')
        step3_dir = os.path.join(output_dir, 'step3_tear_patch_analysis')
        
        os.makedirs(step1_dir, exist_ok=True)
        os.makedirs(step2_dir, exist_ok=True)
        os.makedirs(step3_dir, exist_ok=True)
        
        # è·å–æ‰€æœ‰ROIå›¾åƒæ–‡ä»¶
        roi_pattern = os.path.join(roi_dir, "*_roi.png")
        roi_files = sorted(glob.glob(roi_pattern), key=self.extract_frame_info)
        
        if not roi_files:
            print(f"åœ¨ç›®å½• {roi_dir} ä¸­æœªæ‰¾åˆ°ROIå›¾åƒæ–‡ä»¶")
            return []
        
        print(f"æ‰¾åˆ° {len(roi_files)} ä¸ªROIå›¾åƒæ–‡ä»¶")
        
        results = []
        
        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå‰ªåˆ‡é¢å’Œæ’•è£‚é¢mask
        print("\nç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå‰ªåˆ‡é¢å’Œæ’•è£‚é¢mask...")
        for roi_file in tqdm(roi_files, desc="ç”Ÿæˆå‰ªåˆ‡é¢æ’•è£‚é¢mask", unit="å›¾åƒ"):
            frame_num = self.extract_frame_info(roi_file)
            if frame_num == -1:
                continue
                
            try:
                # è¯»å–ROIå›¾åƒ
                roi_image = cv2.imread(roi_file, cv2.IMREAD_GRAYSCALE)
                if roi_image is None:
                    continue
                
                # ä½¿ç”¨ShearTearDetectoræ£€æµ‹å‰ªåˆ‡é¢å’Œæ’•è£‚é¢
                result = self.shear_tear_detector.detect_surfaces(roi_image, visualize=False)
                if result and 'segmented_image' in result:
                    segmented_image = result['segmented_image']
                    
                    # ä¿å­˜å‰ªåˆ‡é¢mask
                    shear_mask = (segmented_image == 255).astype(np.uint8) * 255
                    shear_filename = f"shear_mask_frame_{frame_num:06d}.png"
                    shear_path = os.path.join(step1_dir, shear_filename)
                    cv2.imwrite(shear_path, shear_mask)
                    
                    # ä¿å­˜æ’•è£‚é¢mask
                    tear_mask = (segmented_image == 128).astype(np.uint8) * 255
                    tear_filename = f"tear_mask_frame_{frame_num:06d}.png"
                    tear_path = os.path.join(step1_dir, tear_filename)
                    cv2.imwrite(tear_path, tear_mask)
                    
            except Exception as e:
                print(f"ç”Ÿæˆmaskæ—¶å‡ºé”™ {roi_file}: {e}")
                continue
        
        print(f"ç¬¬ä¸€æ­¥å®Œæˆï¼Œå‰ªåˆ‡é¢å’Œæ’•è£‚é¢maskå·²ä¿å­˜åˆ°: {step1_dir}")
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨å‰ªåˆ‡é¢maskè¿‡æ»¤å‡ºæ’•è£‚é¢åŒºåŸŸ
        print("\nç¬¬äºŒæ­¥ï¼šè¿‡æ»¤æ’•è£‚é¢åŒºåŸŸ...")
        for roi_file in tqdm(roi_files, desc="è¿‡æ»¤æ’•è£‚é¢åŒºåŸŸ", unit="å›¾åƒ"):
            frame_num = self.extract_frame_info(roi_file)
            if frame_num == -1:
                continue
                
            try:
                # è¯»å–ROIå›¾åƒ
                roi_image = cv2.imread(roi_file, cv2.IMREAD_GRAYSCALE)
                if roi_image is None:
                    continue
                
                # ä½¿ç”¨ShearTearDetectoræ£€æµ‹å‰ªåˆ‡é¢å’Œæ’•è£‚é¢
                result = self.shear_tear_detector.detect_surfaces(roi_image, visualize=False)
                if result and 'segmented_image' in result:
                    segmented_image = result['segmented_image']
                    
                    # ä½¿ç”¨å‰ªåˆ‡é¢maskè¿‡æ»¤å‡ºæ’•è£‚é¢åŒºåŸŸ
                    filtered_tear_region, shear_mask = self.filter_tear_region_with_shear_mask(roi_image, segmented_image)
                    
                    # ä¿å­˜è¿‡æ»¤åçš„æ’•è£‚é¢åŒºåŸŸ
                    filtered_filename = f"filtered_tear_region_frame_{frame_num:06d}.png"
                    filtered_path = os.path.join(step2_dir, filtered_filename)
                    cv2.imwrite(filtered_path, filtered_tear_region)
                    
                    # æ£€æµ‹æ–‘å—
                    spot_result = self.feature_extractor.detect_all_white_spots(filtered_tear_region)
                    
                    # ä¿å­˜æ–‘å—æ£€æµ‹ç»“æœå›¾
                    if 'spot_image' in spot_result:
                        patch_filename = f"tear_patches_frame_{frame_num:06d}.png"
                        patch_path = os.path.join(step2_dir, patch_filename)
                        cv2.imwrite(patch_path, spot_result['spot_image'])
                    
                    # æå–å…³é”®ä¿¡æ¯
                    result_data = {
                        'frame_num': frame_num,
                        'time_seconds': frame_num * 5,  # å‡è®¾æ¯5ç§’ä¸€å¸§
                        'spot_count': spot_result.get('all_spot_count', 0),
                        'spot_density': spot_result.get('all_spot_density', 0.0),
                        'image_shape': roi_image.shape,
                        'roi_file': roi_file
                    }
                    
                    results.append(result_data)
                    
            except Exception as e:
                print(f"åˆ†æROIå›¾åƒ {roi_file} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"ç¬¬äºŒæ­¥å®Œæˆï¼Œè¿‡æ»¤åçš„æ’•è£‚é¢åŒºåŸŸå’Œæ–‘å—å›¾å·²ä¿å­˜åˆ°: {step2_dir}")
        
        # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæ—¶é—´åºåˆ—åˆ†æ
        print("\nç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæ—¶é—´åºåˆ—åˆ†æ...")
        if results:
            # åˆ›å»ºæ—¶é—´åºåˆ—å›¾è¡¨
            plot_path = self.create_temporal_plots(results, step3_dir)
            
            # ä¿å­˜æ•°æ®
            self.save_data_to_csv(results, step3_dir)
            
            # è¾“å‡ºç»Ÿè®¡æ‘˜è¦
            self.print_statistics_summary(results)
            
            print(f"ç¬¬ä¸‰æ­¥å®Œæˆï¼Œåˆ†æç»“æœå’Œæ›²çº¿å›¾å·²ä¿å­˜åˆ°: {step3_dir}")
        
        print(f"æ–‘å—åˆ†æå®Œæˆï¼ŒæˆåŠŸåˆ†æ {len(results)} ä¸ªROIå›¾åƒ")
        return results
    
    def apply_smoothing_filters(self, data: List[Dict[str, Any]], 
                               smoothing_method: str = 'gaussian',
                               window_size: int = 50,
                               sigma: float = 10.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        å¯¹æ—¶é—´åºåˆ—æ•°æ®åº”ç”¨å¹³æ»‘æ»¤æ³¢
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            smoothing_method: å¹³æ»‘æ–¹æ³• ('gaussian', 'moving_avg', 'savgol', 'median')
            window_size: æ»¤æ³¢çª—å£å¤§å°
            sigma: é«˜æ–¯æ»¤æ³¢çš„æ ‡å‡†å·®
            
        Returns:
            æ—¶é—´åºåˆ—ã€å¹³æ»‘åçš„æ–‘å—æ•°é‡ã€å¹³æ»‘åçš„æ–‘å—å¯†åº¦
        """
        time_seconds = np.array([d['time_seconds'] for d in data])
        spot_counts = np.array([d['spot_count'] for d in data])
        spot_densities = np.array([d['spot_density'] for d in data])
        
        if smoothing_method == 'gaussian':
            # é«˜æ–¯æ»¤æ³¢
            smoothed_counts = gaussian_filter1d(spot_counts, sigma=sigma)
            smoothed_densities = gaussian_filter1d(spot_densities, sigma=sigma)
            
        elif smoothing_method == 'moving_avg':
            # ç§»åŠ¨å¹³å‡æ»¤æ³¢
            smoothed_counts = np.convolve(spot_counts, np.ones(window_size)/window_size, mode='same')
            smoothed_densities = np.convolve(spot_densities, np.ones(window_size)/window_size, mode='same')
            
        elif smoothing_method == 'savgol':
            # Savitzky-Golayæ»¤æ³¢
            window_length = min(window_size, len(spot_counts))
            if window_length % 2 == 0:
                window_length -= 1
            smoothed_counts = signal.savgol_filter(spot_counts, window_length, 3)
            smoothed_densities = signal.savgol_filter(spot_densities, window_length, 3)
            
        elif smoothing_method == 'median':
            # ä¸­å€¼æ»¤æ³¢
            smoothed_counts = signal.medfilt(spot_counts, kernel_size=window_size)
            smoothed_densities = signal.medfilt(spot_densities, kernel_size=window_size)
            
        else:
            # é»˜è®¤ä½¿ç”¨é«˜æ–¯æ»¤æ³¢
            smoothed_counts = gaussian_filter1d(spot_counts, sigma=sigma)
            smoothed_densities = gaussian_filter1d(spot_densities, sigma=sigma)
        
        return time_seconds, smoothed_counts, smoothed_densities
    
    def create_temporal_plots(self, data: List[Dict[str, Any]], output_dir: str = "output",
                            smoothing_method: str = 'gaussian', window_size: int = 50, sigma: float = 10.0):
        """
        åˆ›å»ºæ—¶é—´åºåˆ—å›¾è¡¨ï¼ˆå¸¦å¹³æ»‘æ»¤æ³¢ï¼‰
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            output_dir: è¾“å‡ºç›®å½•
            smoothing_method: å¹³æ»‘æ–¹æ³•
            window_size: æ»¤æ³¢çª—å£å¤§å°
            sigma: é«˜æ–¯æ»¤æ³¢æ ‡å‡†å·®
        """
        if not data:
            print("æ²¡æœ‰æ•°æ®å¯ä»¥ç»˜åˆ¶")
            return
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        font_success = setup_chinese_font()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # æå–åŸå§‹æ•°æ®
        time_seconds = np.array([d['time_seconds'] for d in data])
        spot_counts = np.array([d['spot_count'] for d in data])
        spot_densities = np.array([d['spot_density'] for d in data])
        
        # åº”ç”¨å¹³æ»‘æ»¤æ³¢
        _, smoothed_counts, smoothed_densities = self.apply_smoothing_filters(
            data, smoothing_method, window_size, sigma)
        
        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))
        
        # æ’•è£‚é¢æ–‘å—æ•°é‡éšæ—¶é—´å˜åŒ–ï¼ˆåŸå§‹æ•°æ®+å¹³æ»‘æ›²çº¿ï¼‰
        ax1.plot(time_seconds, spot_counts, 'b-', linewidth=0.8, alpha=0.3, label='åŸå§‹æ•°æ®')
        ax1.plot(time_seconds, smoothed_counts, 'b-', linewidth=2.5, alpha=0.9, label='å¹³æ»‘æ›²çº¿')
        ax1.fill_between(time_seconds, smoothed_counts, alpha=0.3, color='blue')
        ax1.set_xlabel('æ—¶é—´ (ç§’)' if font_success else 'Time (seconds)')
        ax1.set_ylabel('æ’•è£‚é¢æ–‘å—æ•°é‡' if font_success else 'Tear Spot Count')
        ax1.set_title('æ’•è£‚é¢æ–‘å—æ•°é‡éšæ—¶é—´å˜åŒ– (å¹³æ»‘æ»¤æ³¢)' if font_success else 'Tear Spot Count Over Time (Smoothed)')
        ax1.grid(True, alpha=0.3)
        ax1.set_xlim(0, max(time_seconds))
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_count = np.mean(spot_counts)
        max_count = np.max(smoothed_counts)
        ax1.axhline(y=mean_count, color='red', linestyle='--', alpha=0.7, 
                   label=f'å¹³å‡å€¼: {mean_count:.1f}')
        ax1.legend()
        
        # æ’•è£‚é¢æ–‘å—å¯†åº¦éšæ—¶é—´å˜åŒ–ï¼ˆåŸå§‹æ•°æ®+å¹³æ»‘æ›²çº¿ï¼‰
        ax2.plot(time_seconds, spot_densities, 'r-', linewidth=0.8, alpha=0.3, label='åŸå§‹æ•°æ®')
        ax2.plot(time_seconds, smoothed_densities, 'r-', linewidth=2.5, alpha=0.9, label='å¹³æ»‘æ›²çº¿')
        ax2.fill_between(time_seconds, smoothed_densities, alpha=0.3, color='red')
        ax2.set_xlabel('æ—¶é—´ (ç§’)' if font_success else 'Time (seconds)')
        ax2.set_ylabel('æ’•è£‚é¢æ–‘å—å¯†åº¦' if font_success else 'Tear Spot Density')
        ax2.set_title('æ’•è£‚é¢æ–‘å—å¯†åº¦éšæ—¶é—´å˜åŒ– (å¹³æ»‘æ»¤æ³¢)' if font_success else 'Tear Spot Density Over Time (Smoothed)')
        ax2.grid(True, alpha=0.3)
        ax2.set_xlim(0, max(time_seconds))
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_density = np.mean(spot_densities)
        max_density = np.max(smoothed_densities)
        ax2.axhline(y=mean_density, color='blue', linestyle='--', alpha=0.7,
                   label=f'å¹³å‡å€¼: {mean_density:.4f}')
        ax2.legend()
        
        # æ·»åŠ æ»¤æ³¢æ–¹æ³•è¯´æ˜
        method_names = {
            'gaussian': 'é«˜æ–¯æ»¤æ³¢',
            'moving_avg': 'ç§»åŠ¨å¹³å‡',
            'savgol': 'Savitzky-Golayæ»¤æ³¢',
            'median': 'ä¸­å€¼æ»¤æ³¢'
        }
        method_name = method_names.get(smoothing_method, smoothing_method)
        fig.suptitle(f'å¹³æ»‘æ–¹æ³•: {method_name} (Ïƒ={sigma}, çª—å£={window_size})', fontsize=12, y=0.98)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.95)
        
        # ä¿å­˜å›¾è¡¨
        plot_path = os.path.join(output_dir, "tear_spot_temporal_analysis_smoothed.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"å¹³æ»‘æ—¶é—´åºåˆ—å›¾è¡¨å·²ä¿å­˜: {plot_path}")
        
        # åˆ›å»ºç»Ÿè®¡æ‘˜è¦å›¾
        self.create_statistics_summary(data, output_dir)
        
        return plot_path
    
    def create_statistics_summary(self, data: List[Dict[str, Any]], output_dir: str):
        """
        åˆ›å»ºç»Ÿè®¡æ‘˜è¦å›¾è¡¨
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        if not data:
            return
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        font_success = setup_chinese_font()
        
        # æå–æ•°æ®
        spot_counts = [d['spot_count'] for d in data]
        spot_densities = [d['spot_density'] for d in data]
        
        # åˆ›å»ºç»Ÿè®¡å›¾è¡¨
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # æ’•è£‚é¢æ–‘å—æ•°é‡åˆ†å¸ƒç›´æ–¹å›¾
        ax1.hist(spot_counts, bins=30, alpha=0.7, color='blue', edgecolor='black')
        ax1.set_xlabel('æ’•è£‚é¢æ–‘å—æ•°é‡' if font_success else 'Tear Spot Count')
        ax1.set_ylabel('é¢‘æ¬¡' if font_success else 'Frequency')
        ax1.set_title('æ’•è£‚é¢æ–‘å—æ•°é‡åˆ†å¸ƒ' if font_success else 'Tear Spot Count Distribution')
        ax1.grid(True, alpha=0.3)
        
        # æ’•è£‚é¢æ–‘å—å¯†åº¦åˆ†å¸ƒç›´æ–¹å›¾
        ax2.hist(spot_densities, bins=30, alpha=0.7, color='red', edgecolor='black')
        ax2.set_xlabel('æ’•è£‚é¢æ–‘å—å¯†åº¦' if font_success else 'Tear Spot Density')
        ax2.set_ylabel('é¢‘æ¬¡' if font_success else 'Frequency')
        ax2.set_title('æ’•è£‚é¢æ–‘å—å¯†åº¦åˆ†å¸ƒ' if font_success else 'Tear Spot Density Distribution')
        ax2.grid(True, alpha=0.3)
        
        # æ’•è£‚é¢æ–‘å—æ•°é‡ä¸å¯†åº¦çš„æ•£ç‚¹å›¾
        ax3.scatter(spot_counts, spot_densities, alpha=0.6, color='green')
        ax3.set_xlabel('æ’•è£‚é¢æ–‘å—æ•°é‡' if font_success else 'Tear Spot Count')
        ax3.set_ylabel('æ’•è£‚é¢æ–‘å—å¯†åº¦' if font_success else 'Tear Spot Density')
        ax3.set_title('æ’•è£‚é¢æ–‘å—æ•°é‡ä¸å¯†åº¦å…³ç³»' if font_success else 'Tear Spot Count vs Density')
        ax3.grid(True, alpha=0.3)
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = np.corrcoef(spot_counts, spot_densities)[0, 1]
        ax3.text(0.05, 0.95, f'ç›¸å…³ç³»æ•°: {correlation:.3f}', 
                transform=ax3.transAxes, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        # ç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
        ax4.axis('off')
        stats_text = f"""
ç»Ÿè®¡æ‘˜è¦:

æ’•è£‚é¢æ–‘å—æ•°é‡:
  å¹³å‡å€¼: {np.mean(spot_counts):.2f}
  æ ‡å‡†å·®: {np.std(spot_counts):.2f}
  æœ€å°å€¼: {np.min(spot_counts)}
  æœ€å¤§å€¼: {np.max(spot_counts)}
  ä¸­ä½æ•°: {np.median(spot_counts):.2f}

æ’•è£‚é¢æ–‘å—å¯†åº¦:
  å¹³å‡å€¼: {np.mean(spot_densities):.6f}
  æ ‡å‡†å·®: {np.std(spot_densities):.6f}
  æœ€å°å€¼: {np.min(spot_densities):.6f}
  æœ€å¤§å€¼: {np.max(spot_densities):.6f}
  ä¸­ä½æ•°: {np.median(spot_densities):.6f}

æ•°æ®ç‚¹æ€»æ•°: {len(data)}
        """
        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜ç»Ÿè®¡å›¾è¡¨
        stats_path = os.path.join(output_dir, "tear_spot_statistics_summary.png")
        plt.savefig(stats_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ç»Ÿè®¡æ‘˜è¦å›¾è¡¨å·²ä¿å­˜: {stats_path}")
    
    def save_data_to_csv(self, data: List[Dict[str, Any]], output_dir: str = "output"):
        """
        ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        if not data:
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(data)
        
        # ä¿å­˜CSVæ–‡ä»¶
        csv_path = os.path.join(output_dir, "tear_spot_temporal_data.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8')
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {csv_path}")
        
        # ä¿å­˜JSONæ ¼å¼
        json_path = os.path.join(output_dir, "tear_spot_temporal_data.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ°JSONæ–‡ä»¶: {json_path}")
    
    def run_analysis(self, roi_dir: str = "data/roi_imgs", output_dir: str = "output",
                    smoothing_method: str = 'gaussian', window_size: int = 50, sigma: float = 10.0):
        """
        è¿è¡Œå®Œæ•´çš„æ’•è£‚é¢æ–‘å—æ—¶é—´åºåˆ—åˆ†æï¼ˆä½¿ç”¨å‰ªåˆ‡é¢è¿‡æ»¤ï¼‰
        
        Args:
            roi_dir: ROIå›¾åƒç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            smoothing_method: å¹³æ»‘æ–¹æ³• ('gaussian', 'moving_avg', 'savgol', 'median')
            window_size: æ»¤æ³¢çª—å£å¤§å°
            sigma: é«˜æ–¯æ»¤æ³¢æ ‡å‡†å·®
        """
        print("=== æ’•è£‚é¢æ–‘å—æ—¶é—´åºåˆ—åˆ†æ (ä½¿ç”¨å‰ªåˆ‡é¢è¿‡æ»¤) ===")
        
        # åˆ†æROIå›¾åƒçš„æ’•è£‚é¢æ–‘å—ç‰¹å¾
        data = self.analyze_roi_spots_with_tear_filter(roi_dir, output_dir)
        
        if not data:
            print("æ²¡æœ‰å¯åˆ†æçš„æ•°æ®")
            return
        
        print(f"\nâœ… æ’•è£‚é¢æ–‘å—æ—¶é—´åºåˆ—åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š åˆ†æç»“æœä¿å­˜ä½ç½®: {output_dir}")
        print(f"ğŸ”§ å¹³æ»‘æ–¹æ³•: {smoothing_method}, çª—å£å¤§å°: {window_size}, Ïƒ: {sigma}")
        
        return data
    
    def print_statistics_summary(self, data: List[Dict[str, Any]]):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        if not data:
            return
        
        spot_counts = [d['spot_count'] for d in data]
        spot_densities = [d['spot_density'] for d in data]
        
        print("\n=== æ’•è£‚é¢æ–‘å—æ—¶é—´åºåˆ—ç»Ÿè®¡æ‘˜è¦ ===")
        print(f"æ•°æ®ç‚¹æ€»æ•°: {len(data)}")
        print(f"æ—¶é—´è·¨åº¦: {data[0]['time_seconds']:.1f} - {data[-1]['time_seconds']:.1f} ç§’")
        print(f"å¸§æ•°èŒƒå›´: {data[0]['frame_num']} - {data[-1]['frame_num']}")
        
        print("\næ’•è£‚é¢æ–‘å—æ•°é‡ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {np.mean(spot_counts):.2f}")
        print(f"  æ ‡å‡†å·®: {np.std(spot_counts):.2f}")
        print(f"  æœ€å°å€¼: {np.min(spot_counts)}")
        print(f"  æœ€å¤§å€¼: {np.max(spot_counts)}")
        print(f"  ä¸­ä½æ•°: {np.median(spot_counts):.2f}")
        
        print("\næ’•è£‚é¢æ–‘å—å¯†åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {np.mean(spot_densities):.6f}")
        print(f"  æ ‡å‡†å·®: {np.std(spot_densities):.6f}")
        print(f"  æœ€å°å€¼: {np.min(spot_densities):.6f}")
        print(f"  æœ€å¤§å€¼: {np.max(spot_densities):.6f}")
        print(f"  ä¸­ä½æ•°: {np.median(spot_densities):.6f}")
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = np.corrcoef(spot_counts, spot_densities)[0, 1]
        print(f"\næ’•è£‚é¢æ–‘å—æ•°é‡ä¸å¯†åº¦ç›¸å…³ç³»æ•°: {correlation:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = TearFilterTemporalAnalyzer()
    
    # è¿è¡Œåˆ†æï¼ˆä½¿ç”¨é«˜æ–¯æ»¤æ³¢ï¼ŒÏƒ=10ï¼Œçª—å£å¤§å°50ï¼‰
    data = analyzer.run_analysis(
        roi_dir="data/roi_imgs",
        output_dir="output/tear_filter_temporal_analysis",
        smoothing_method='gaussian',
        window_size=50,
        sigma=10.0
    )
    
    if data:
        print(f"\nğŸ¯ åˆ†æå®Œæˆï¼å…±åˆ†æäº† {len(data)} ä¸ªæ—¶é—´ç‚¹çš„æ’•è£‚é¢æ–‘å—æ•°æ®")
        print("ğŸ“ˆ ç”Ÿæˆäº†å¹³æ»‘æ—¶é—´åºåˆ—æ›²çº¿å›¾å’Œç»Ÿè®¡æ‘˜è¦")


if __name__ == "__main__":
    main()
