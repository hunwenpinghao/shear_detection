#!/usr/bin/env python3
"""
æ’•è£‚é¢æ–‘å—æ•°é‡å’Œå¯†åº¦æ—¶é—´åºåˆ—åˆ†æ
ä¸»è¦åŠŸèƒ½ï¼š
1. ä»æ–‘å—æ£€æµ‹ç»“æœä¸­æå–æ–‘å—æ•°é‡å’Œå¯†åº¦æ•°æ®
2. ç»˜åˆ¶æ–‘å—æ•°é‡å’Œå¯†åº¦éšæ—¶é—´å˜åŒ–çš„æ›²çº¿å›¾
3. ç”Ÿæˆæ—¶é—´åºåˆ—åˆ†ææŠ¥å‘Š
"""

import cv2
import numpy as np
import os
import glob
import sys
import matplotlib.pyplot as plt
import pandas as pd
from typing import List, Dict, Any, Tuple
import json
import platform
from scipy import signal
from scipy.ndimage import gaussian_filter1d

# æ·»åŠ data_processç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'data_process'))

from feature_extractor import FeatureExtractor
from config import PREPROCESS_CONFIG

# è®¾ç½®ä¸­æ–‡å­—ä½“
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    import matplotlib.font_manager as fm
    
    system = platform.system()
    
    if system == "Darwin":  # macOS
        available_fonts = [f.name for f in fm.fontManager.ttflist]
        chinese_fonts = []
        
        preferred_fonts = ['PingFang SC', 'Hiragino Sans GB', 'STHeiti', 'Arial Unicode MS']
        
        for font in preferred_fonts:
            if font in available_fonts:
                chinese_fonts.append(font)
        
        if chinese_fonts:
            plt.rcParams['font.sans-serif'] = chinese_fonts + ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            print(f"è®¾ç½®ä¸­æ–‡å­—ä½“: {chinese_fonts[0]}")
            return True
    
    elif system == "Windows":
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        print("è®¾ç½®Windowsä¸­æ–‡å­—ä½“")
        return True
    
    else:  # Linux
        plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'Droid Sans Fallback', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        print("è®¾ç½®Linuxä¸­æ–‡å­—ä½“")
        return True
    
    print("æ— æ³•è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
    return False

class SpotTemporalAnalyzer:
    """æ–‘å—æ—¶é—´åºåˆ—åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.feature_extractor = FeatureExtractor(PREPROCESS_CONFIG)
        self.data = []
        
    def extract_frame_info(self, filename: str) -> int:
        """ä»æ–‡ä»¶åæå–å¸§å·"""
        try:
            basename = os.path.basename(filename)
            # æå–frame_XXXXXXä¸­çš„æ•°å­—
            frame_num = int(basename.split('_')[1])
            return frame_num
        except (IndexError, ValueError):
            return -1
    
    def analyze_roi_spots(self, roi_dir: str) -> List[Dict[str, Any]]:
        """
        åˆ†æROIå›¾åƒçš„æ–‘å—ç‰¹å¾
        
        Args:
            roi_dir: ROIå›¾åƒç›®å½•è·¯å¾„
            
        Returns:
            æ–‘å—åˆ†æç»“æœåˆ—è¡¨
        """
        print("å¼€å§‹åˆ†æROIå›¾åƒçš„æ–‘å—ç‰¹å¾...")
        
        # è·å–æ‰€æœ‰ROIå›¾åƒæ–‡ä»¶
        roi_pattern = os.path.join(roi_dir, "*_roi.png")
        roi_files = sorted(glob.glob(roi_pattern), key=self.extract_frame_info)
        
        if not roi_files:
            print(f"åœ¨ç›®å½• {roi_dir} ä¸­æœªæ‰¾åˆ°ROIå›¾åƒæ–‡ä»¶")
            return []
        
        print(f"æ‰¾åˆ° {len(roi_files)} ä¸ªROIå›¾åƒæ–‡ä»¶")
        
        results = []
        for i, roi_file in enumerate(roi_files):
            frame_num = self.extract_frame_info(roi_file)
            if frame_num == -1:
                continue
                
            try:
                # è¯»å–ROIå›¾åƒ
                roi_image = cv2.imread(roi_file, cv2.IMREAD_GRAYSCALE)
                if roi_image is None:
                    continue
                
                # æ£€æµ‹æ–‘å—
                spot_result = self.feature_extractor.detect_all_white_spots(roi_image)
                
                # æå–å…³é”®ä¿¡æ¯
                result = {
                    'frame_num': frame_num,
                    'time_seconds': frame_num * 5,  # å‡è®¾æ¯5ç§’ä¸€å¸§
                    'spot_count': spot_result.get('all_spot_count', 0),
                    'spot_density': spot_result.get('all_spot_density', 0.0),
                    'image_shape': roi_image.shape,
                    'roi_file': roi_file
                }
                
                results.append(result)
                
                # æ¯100å¸§è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if (i + 1) % 100 == 0:
                    print(f"å·²åˆ†æ {i + 1}/{len(roi_files)} ä¸ªROIå›¾åƒ")
                    
            except Exception as e:
                print(f"åˆ†æROIå›¾åƒ {roi_file} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"æ–‘å—åˆ†æå®Œæˆï¼ŒæˆåŠŸåˆ†æ {len(results)} ä¸ªROIå›¾åƒ")
        return results
    
    def apply_smoothing_filters(self, data: List[Dict[str, Any]], 
                               smoothing_method: str = 'gaussian',
                               window_size: int = 50,
                               sigma: float = 10.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        å¯¹æ—¶é—´åºåˆ—æ•°æ®åº”ç”¨å¹³æ»‘æ»¤æ³¢
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            smoothing_method: å¹³æ»‘æ–¹æ³• ('gaussian', 'moving_avg', 'savgol', 'median')
            window_size: æ»¤æ³¢çª—å£å¤§å°
            sigma: é«˜æ–¯æ»¤æ³¢çš„æ ‡å‡†å·®
            
        Returns:
            æ—¶é—´åºåˆ—ã€å¹³æ»‘åçš„æ–‘å—æ•°é‡ã€å¹³æ»‘åçš„æ–‘å—å¯†åº¦
        """
        time_seconds = np.array([d['time_seconds'] for d in data])
        spot_counts = np.array([d['spot_count'] for d in data])
        spot_densities = np.array([d['spot_density'] for d in data])
        
        if smoothing_method == 'gaussian':
            # é«˜æ–¯æ»¤æ³¢
            smoothed_counts = gaussian_filter1d(spot_counts, sigma=sigma)
            smoothed_densities = gaussian_filter1d(spot_densities, sigma=sigma)
            
        elif smoothing_method == 'moving_avg':
            # ç§»åŠ¨å¹³å‡æ»¤æ³¢
            smoothed_counts = np.convolve(spot_counts, np.ones(window_size)/window_size, mode='same')
            smoothed_densities = np.convolve(spot_densities, np.ones(window_size)/window_size, mode='same')
            
        elif smoothing_method == 'savgol':
            # Savitzky-Golayæ»¤æ³¢
            window_length = min(window_size, len(spot_counts))
            if window_length % 2 == 0:
                window_length -= 1
            smoothed_counts = signal.savgol_filter(spot_counts, window_length, 3)
            smoothed_densities = signal.savgol_filter(spot_densities, window_length, 3)
            
        elif smoothing_method == 'median':
            # ä¸­å€¼æ»¤æ³¢
            smoothed_counts = signal.medfilt(spot_counts, kernel_size=window_size)
            smoothed_densities = signal.medfilt(spot_densities, kernel_size=window_size)
            
        else:
            # é»˜è®¤ä½¿ç”¨é«˜æ–¯æ»¤æ³¢
            smoothed_counts = gaussian_filter1d(spot_counts, sigma=sigma)
            smoothed_densities = gaussian_filter1d(spot_densities, sigma=sigma)
        
        return time_seconds, smoothed_counts, smoothed_densities
    
    def create_temporal_plots(self, data: List[Dict[str, Any]], output_dir: str = "output",
                            smoothing_method: str = 'gaussian', window_size: int = 50, sigma: float = 10.0):
        """
        åˆ›å»ºæ—¶é—´åºåˆ—å›¾è¡¨ï¼ˆå¸¦å¹³æ»‘æ»¤æ³¢ï¼‰
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            output_dir: è¾“å‡ºç›®å½•
            smoothing_method: å¹³æ»‘æ–¹æ³•
            window_size: æ»¤æ³¢çª—å£å¤§å°
            sigma: é«˜æ–¯æ»¤æ³¢æ ‡å‡†å·®
        """
        if not data:
            print("æ²¡æœ‰æ•°æ®å¯ä»¥ç»˜åˆ¶")
            return
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        font_success = setup_chinese_font()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # æå–åŸå§‹æ•°æ®
        time_seconds = np.array([d['time_seconds'] for d in data])
        spot_counts = np.array([d['spot_count'] for d in data])
        spot_densities = np.array([d['spot_density'] for d in data])
        
        # åº”ç”¨å¹³æ»‘æ»¤æ³¢
        _, smoothed_counts, smoothed_densities = self.apply_smoothing_filters(
            data, smoothing_method, window_size, sigma)
        
        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))
        
        # æ–‘å—æ•°é‡éšæ—¶é—´å˜åŒ–ï¼ˆåŸå§‹æ•°æ®+å¹³æ»‘æ›²çº¿ï¼‰
        ax1.plot(time_seconds, spot_counts, 'b-', linewidth=0.8, alpha=0.3, label='åŸå§‹æ•°æ®')
        ax1.plot(time_seconds, smoothed_counts, 'b-', linewidth=2.5, alpha=0.9, label='å¹³æ»‘æ›²çº¿')
        ax1.fill_between(time_seconds, smoothed_counts, alpha=0.3, color='blue')
        ax1.set_xlabel('æ—¶é—´ (ç§’)' if font_success else 'Time (seconds)')
        ax1.set_ylabel('æ–‘å—æ•°é‡' if font_success else 'Spot Count')
        ax1.set_title('æ–‘å—æ•°é‡éšæ—¶é—´å˜åŒ– (å¹³æ»‘æ»¤æ³¢)' if font_success else 'Spot Count Over Time (Smoothed)')
        ax1.grid(True, alpha=0.3)
        ax1.set_xlim(0, max(time_seconds))
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_count = np.mean(spot_counts)
        max_count = np.max(smoothed_counts)
        ax1.axhline(y=mean_count, color='red', linestyle='--', alpha=0.7, 
                   label=f'å¹³å‡å€¼: {mean_count:.1f}')
        ax1.legend()
        
        # æ–‘å—å¯†åº¦éšæ—¶é—´å˜åŒ–ï¼ˆåŸå§‹æ•°æ®+å¹³æ»‘æ›²çº¿ï¼‰
        ax2.plot(time_seconds, spot_densities, 'r-', linewidth=0.8, alpha=0.3, label='åŸå§‹æ•°æ®')
        ax2.plot(time_seconds, smoothed_densities, 'r-', linewidth=2.5, alpha=0.9, label='å¹³æ»‘æ›²çº¿')
        ax2.fill_between(time_seconds, smoothed_densities, alpha=0.3, color='red')
        ax2.set_xlabel('æ—¶é—´ (ç§’)' if font_success else 'Time (seconds)')
        ax2.set_ylabel('æ–‘å—å¯†åº¦' if font_success else 'Spot Density')
        ax2.set_title('æ–‘å—å¯†åº¦éšæ—¶é—´å˜åŒ– (å¹³æ»‘æ»¤æ³¢)' if font_success else 'Spot Density Over Time (Smoothed)')
        ax2.grid(True, alpha=0.3)
        ax2.set_xlim(0, max(time_seconds))
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_density = np.mean(spot_densities)
        max_density = np.max(smoothed_densities)
        ax2.axhline(y=mean_density, color='blue', linestyle='--', alpha=0.7,
                   label=f'å¹³å‡å€¼: {mean_density:.4f}')
        ax2.legend()
        
        # æ·»åŠ æ»¤æ³¢æ–¹æ³•è¯´æ˜
        method_names = {
            'gaussian': 'é«˜æ–¯æ»¤æ³¢',
            'moving_avg': 'ç§»åŠ¨å¹³å‡',
            'savgol': 'Savitzky-Golayæ»¤æ³¢',
            'median': 'ä¸­å€¼æ»¤æ³¢'
        }
        method_name = method_names.get(smoothing_method, smoothing_method)
        fig.suptitle(f'å¹³æ»‘æ–¹æ³•: {method_name} (Ïƒ={sigma}, çª—å£={window_size})', fontsize=12, y=0.98)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.95)
        
        # ä¿å­˜å›¾è¡¨
        plot_path = os.path.join(output_dir, "spot_temporal_analysis_smoothed.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"å¹³æ»‘æ—¶é—´åºåˆ—å›¾è¡¨å·²ä¿å­˜: {plot_path}")
        
        # åˆ›å»ºç»Ÿè®¡æ‘˜è¦å›¾
        self.create_statistics_summary(data, output_dir)
        
        return plot_path
    
    def create_statistics_summary(self, data: List[Dict[str, Any]], output_dir: str):
        """
        åˆ›å»ºç»Ÿè®¡æ‘˜è¦å›¾è¡¨
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        if not data:
            return
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        font_success = setup_chinese_font()
        
        # æå–æ•°æ®
        spot_counts = [d['spot_count'] for d in data]
        spot_densities = [d['spot_density'] for d in data]
        
        # åˆ›å»ºç»Ÿè®¡å›¾è¡¨
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # æ–‘å—æ•°é‡åˆ†å¸ƒç›´æ–¹å›¾
        ax1.hist(spot_counts, bins=30, alpha=0.7, color='blue', edgecolor='black')
        ax1.set_xlabel('æ–‘å—æ•°é‡' if font_success else 'Spot Count')
        ax1.set_ylabel('é¢‘æ¬¡' if font_success else 'Frequency')
        ax1.set_title('æ–‘å—æ•°é‡åˆ†å¸ƒ' if font_success else 'Spot Count Distribution')
        ax1.grid(True, alpha=0.3)
        
        # æ–‘å—å¯†åº¦åˆ†å¸ƒç›´æ–¹å›¾
        ax2.hist(spot_densities, bins=30, alpha=0.7, color='red', edgecolor='black')
        ax2.set_xlabel('æ–‘å—å¯†åº¦' if font_success else 'Spot Density')
        ax2.set_ylabel('é¢‘æ¬¡' if font_success else 'Frequency')
        ax2.set_title('æ–‘å—å¯†åº¦åˆ†å¸ƒ' if font_success else 'Spot Density Distribution')
        ax2.grid(True, alpha=0.3)
        
        # æ–‘å—æ•°é‡ä¸å¯†åº¦çš„æ•£ç‚¹å›¾
        ax3.scatter(spot_counts, spot_densities, alpha=0.6, color='green')
        ax3.set_xlabel('æ–‘å—æ•°é‡' if font_success else 'Spot Count')
        ax3.set_ylabel('æ–‘å—å¯†åº¦' if font_success else 'Spot Density')
        ax3.set_title('æ–‘å—æ•°é‡ä¸å¯†åº¦å…³ç³»' if font_success else 'Spot Count vs Density')
        ax3.grid(True, alpha=0.3)
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = np.corrcoef(spot_counts, spot_densities)[0, 1]
        ax3.text(0.05, 0.95, f'ç›¸å…³ç³»æ•°: {correlation:.3f}', 
                transform=ax3.transAxes, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        # ç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
        ax4.axis('off')
        stats_text = f"""
ç»Ÿè®¡æ‘˜è¦:

æ–‘å—æ•°é‡:
  å¹³å‡å€¼: {np.mean(spot_counts):.2f}
  æ ‡å‡†å·®: {np.std(spot_counts):.2f}
  æœ€å°å€¼: {np.min(spot_counts)}
  æœ€å¤§å€¼: {np.max(spot_counts)}
  ä¸­ä½æ•°: {np.median(spot_counts):.2f}

æ–‘å—å¯†åº¦:
  å¹³å‡å€¼: {np.mean(spot_densities):.6f}
  æ ‡å‡†å·®: {np.std(spot_densities):.6f}
  æœ€å°å€¼: {np.min(spot_densities):.6f}
  æœ€å¤§å€¼: {np.max(spot_densities):.6f}
  ä¸­ä½æ•°: {np.median(spot_densities):.6f}

æ•°æ®ç‚¹æ€»æ•°: {len(data)}
        """
        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜ç»Ÿè®¡å›¾è¡¨
        stats_path = os.path.join(output_dir, "spot_statistics_summary.png")
        plt.savefig(stats_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ç»Ÿè®¡æ‘˜è¦å›¾è¡¨å·²ä¿å­˜: {stats_path}")
    
    def save_data_to_csv(self, data: List[Dict[str, Any]], output_dir: str = "output"):
        """
        ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        if not data:
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(data)
        
        # ä¿å­˜CSVæ–‡ä»¶
        csv_path = os.path.join(output_dir, "spot_temporal_data.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8')
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {csv_path}")
        
        # ä¿å­˜JSONæ ¼å¼
        json_path = os.path.join(output_dir, "spot_temporal_data.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ°JSONæ–‡ä»¶: {json_path}")
    
    def run_analysis(self, roi_dir: str = "data/roi_imgs", output_dir: str = "output",
                    smoothing_method: str = 'gaussian', window_size: int = 50, sigma: float = 10.0):
        """
        è¿è¡Œå®Œæ•´çš„æ–‘å—æ—¶é—´åºåˆ—åˆ†æï¼ˆå¸¦å¹³æ»‘æ»¤æ³¢ï¼‰
        
        Args:
            roi_dir: ROIå›¾åƒç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            smoothing_method: å¹³æ»‘æ–¹æ³• ('gaussian', 'moving_avg', 'savgol', 'median')
            window_size: æ»¤æ³¢çª—å£å¤§å°
            sigma: é«˜æ–¯æ»¤æ³¢æ ‡å‡†å·®
        """
        print("=== æ’•è£‚é¢æ–‘å—æ—¶é—´åºåˆ—åˆ†æ (å¹³æ»‘æ»¤æ³¢) ===")
        
        # åˆ†æROIå›¾åƒçš„æ–‘å—ç‰¹å¾
        data = self.analyze_roi_spots(roi_dir)
        
        if not data:
            print("æ²¡æœ‰å¯åˆ†æçš„æ•°æ®")
            return
        
        # åˆ›å»ºæ—¶é—´åºåˆ—å›¾è¡¨ï¼ˆå¸¦å¹³æ»‘æ»¤æ³¢ï¼‰
        plot_path = self.create_temporal_plots(data, output_dir, smoothing_method, window_size, sigma)
        
        # ä¿å­˜æ•°æ®
        self.save_data_to_csv(data, output_dir)
        
        # è¾“å‡ºç»Ÿè®¡æ‘˜è¦
        self.print_statistics_summary(data)
        
        print(f"\nâœ… æ–‘å—æ—¶é—´åºåˆ—åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š å¹³æ»‘å›¾è¡¨ä¿å­˜ä½ç½®: {plot_path}")
        print(f"ğŸ“ æ•°æ®ä¿å­˜ä½ç½®: {output_dir}")
        print(f"ğŸ”§ å¹³æ»‘æ–¹æ³•: {smoothing_method}, çª—å£å¤§å°: {window_size}, Ïƒ: {sigma}")
        
        return data
    
    def print_statistics_summary(self, data: List[Dict[str, Any]]):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        if not data:
            return
        
        spot_counts = [d['spot_count'] for d in data]
        spot_densities = [d['spot_density'] for d in data]
        
        print("\n=== æ–‘å—æ—¶é—´åºåˆ—ç»Ÿè®¡æ‘˜è¦ ===")
        print(f"æ•°æ®ç‚¹æ€»æ•°: {len(data)}")
        print(f"æ—¶é—´è·¨åº¦: {data[0]['time_seconds']:.1f} - {data[-1]['time_seconds']:.1f} ç§’")
        print(f"å¸§æ•°èŒƒå›´: {data[0]['frame_num']} - {data[-1]['frame_num']}")
        
        print("\næ–‘å—æ•°é‡ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {np.mean(spot_counts):.2f}")
        print(f"  æ ‡å‡†å·®: {np.std(spot_counts):.2f}")
        print(f"  æœ€å°å€¼: {np.min(spot_counts)}")
        print(f"  æœ€å¤§å€¼: {np.max(spot_counts)}")
        print(f"  ä¸­ä½æ•°: {np.median(spot_counts):.2f}")
        
        print("\næ–‘å—å¯†åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {np.mean(spot_densities):.6f}")
        print(f"  æ ‡å‡†å·®: {np.std(spot_densities):.6f}")
        print(f"  æœ€å°å€¼: {np.min(spot_densities):.6f}")
        print(f"  æœ€å¤§å€¼: {np.max(spot_densities):.6f}")
        print(f"  ä¸­ä½æ•°: {np.median(spot_densities):.6f}")
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = np.corrcoef(spot_counts, spot_densities)[0, 1]
        print(f"\næ–‘å—æ•°é‡ä¸å¯†åº¦ç›¸å…³ç³»æ•°: {correlation:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = SpotTemporalAnalyzer()
    
    # è¿è¡Œåˆ†æï¼ˆä½¿ç”¨é«˜æ–¯æ»¤æ³¢ï¼ŒÏƒ=10ï¼Œçª—å£å¤§å°50ï¼‰
    data = analyzer.run_analysis(
        roi_dir="data/roi_imgs",
        output_dir="output/temporal_analysis",
        smoothing_method='gaussian',
        window_size=50,
        sigma=10.0
    )
    
    if data:
        print(f"\nğŸ¯ åˆ†æå®Œæˆï¼å…±åˆ†æäº† {len(data)} ä¸ªæ—¶é—´ç‚¹çš„æ–‘å—æ•°æ®")
        print("ğŸ“ˆ ç”Ÿæˆäº†å¹³æ»‘æ—¶é—´åºåˆ—æ›²çº¿å›¾å’Œç»Ÿè®¡æ‘˜è¦")


if __name__ == "__main__":
    main()
