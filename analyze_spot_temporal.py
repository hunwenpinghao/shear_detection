#!/usr/bin/env python3
"""
æ’•è£‚é¢æ–‘å—æ•°é‡å’Œå¯†åº¦æ—¶é—´åºåˆ—åˆ†æ
ä¸»è¦åŠŸèƒ½ï¼š
1. ä»æ–‘å—æ£€æµ‹ç»“æœä¸­æå–æ–‘å—æ•°é‡å’Œå¯†åº¦æ•°æ®
2. ç»˜åˆ¶æ–‘å—æ•°é‡å’Œå¯†åº¦éšæ—¶é—´å˜åŒ–çš„æ›²çº¿å›¾
3. ç”Ÿæˆæ—¶é—´åºåˆ—åˆ†ææŠ¥å‘Š
4. æ”¯æŒå‘½ä»¤è¡Œå‚æ•°é…ç½®

ç”¨æ³•:
    python analyze_spot_temporal.py --roi_dir data/roi_imgs --output_dir output
    
ç¤ºä¾‹:
    # åŸºæœ¬ç”¨æ³•
    python analyze_spot_temporal.py --roi_dir data/roi_imgs --output_dir output/spot_analysis
    
    # è‡ªå®šä¹‰å¹³æ»‘å‚æ•°
    python analyze_spot_temporal.py --roi_dir data/roi_imgs --output_dir output \
        --smoothing_method savgol --window_size 100 --sigma 15.0
    
    # ä¿å­˜æ¯ä¸€å¸§çš„å¯è§†åŒ–ï¼ˆæ¯100å¸§ï¼‰
    python analyze_spot_temporal.py --roi_dir data/roi_imgs --output_dir output \
        --viz_interval 100
"""

import cv2
import numpy as np
import os
import glob
import sys
import argparse
import matplotlib.pyplot as plt
import pandas as pd
from typing import List, Dict, Any, Tuple
import json
import platform
from scipy import signal
from scipy.ndimage import gaussian_filter1d
from tqdm import tqdm

# æ·»åŠ data_processç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'data_process'))

from feature_extractor import FeatureExtractor
from config import PREPROCESS_CONFIG

# è®¾ç½®ä¸­æ–‡å­—ä½“
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    import matplotlib.font_manager as fm
    
    system = platform.system()
    
    if system == "Darwin":  # macOS
        available_fonts = [f.name for f in fm.fontManager.ttflist]
        chinese_fonts = []
        
        preferred_fonts = ['PingFang SC', 'Hiragino Sans GB', 'STHeiti', 'Arial Unicode MS']
        
        for font in preferred_fonts:
            if font in available_fonts:
                chinese_fonts.append(font)
        
        if chinese_fonts:
            plt.rcParams['font.sans-serif'] = chinese_fonts + ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            print(f"è®¾ç½®ä¸­æ–‡å­—ä½“: {chinese_fonts[0]}")
            return True
    
    elif system == "Windows":
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        print("è®¾ç½®Windowsä¸­æ–‡å­—ä½“")
        return True
    
    else:  # Linux
        plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'Droid Sans Fallback', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        print("è®¾ç½®Linuxä¸­æ–‡å­—ä½“")
        return True
    
    print("æ— æ³•è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
    return False

class SpotTemporalAnalyzer:
    """æ–‘å—æ—¶é—´åºåˆ—åˆ†æå™¨"""
    
    def __init__(self, viz_interval: int = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            viz_interval: å¯è§†åŒ–é‡‡æ ·é—´éš”ï¼Œæ¯éš”å¤šå°‘å¸§ä¿å­˜ä¸€æ¬¡æ ‡æ³¨å›¾ï¼ˆé»˜è®¤None=ä¸ä¿å­˜ï¼‰
        """
        self.feature_extractor = FeatureExtractor(PREPROCESS_CONFIG)
        self.data = []
        self.viz_interval = viz_interval
        
    def extract_frame_info(self, filename: str) -> int:
        """ä»æ–‡ä»¶åæå–å¸§å·"""
        try:
            basename = os.path.basename(filename)
            # æå–frame_XXXXXXä¸­çš„æ•°å­—
            frame_num = int(basename.split('_')[1])
            return frame_num
        except (IndexError, ValueError):
            return -1
    
    def analyze_roi_spots(self, roi_dir: str, output_dir: str = "output") -> List[Dict[str, Any]]:
        """
        åˆ†æROIå›¾åƒçš„æ–‘å—ç‰¹å¾
        
        Args:
            roi_dir: ROIå›¾åƒç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼ˆç”¨äºä¿å­˜å¯è§†åŒ–ï¼‰
            
        Returns:
            æ–‘å—åˆ†æç»“æœåˆ—è¡¨
        """
        print("å¼€å§‹åˆ†æROIå›¾åƒçš„æ–‘å—ç‰¹å¾...")
        if self.viz_interval:
            print(f"å¯è§†åŒ–é‡‡æ ·é—´éš”: æ¯ {self.viz_interval} å¸§")
        
        # è·å–æ‰€æœ‰ROIå›¾åƒæ–‡ä»¶
        roi_pattern = os.path.join(roi_dir, "*_roi.png")
        roi_files = sorted(glob.glob(roi_pattern), key=self.extract_frame_info)
        
        if not roi_files:
            print(f"åœ¨ç›®å½• {roi_dir} ä¸­æœªæ‰¾åˆ°ROIå›¾åƒæ–‡ä»¶")
            return []
        
        print(f"æ‰¾åˆ° {len(roi_files)} ä¸ªROIå›¾åƒæ–‡ä»¶")
        
        # åˆ›å»ºå¯è§†åŒ–ç›®å½•
        viz_count = 0
        if self.viz_interval:
            viz_dir = os.path.join(output_dir, 'spot_visualizations')
            os.makedirs(viz_dir, exist_ok=True)
        
        results = []
        for i, roi_file in enumerate(tqdm(roi_files, desc="åˆ†ææ–‘å—")):
            frame_num = self.extract_frame_info(roi_file)
            if frame_num == -1:
                continue
                
            try:
                # è¯»å–ROIå›¾åƒ
                roi_image = cv2.imread(roi_file, cv2.IMREAD_GRAYSCALE)
                if roi_image is None:
                    continue
                
                # æ£€æµ‹æ–‘å—
                spot_result = self.feature_extractor.detect_all_white_spots(roi_image)
                
                # æå–å…³é”®ä¿¡æ¯
                result = {
                    'frame_num': frame_num,
                    'time_seconds': frame_num * 5,  # å‡è®¾æ¯5ç§’ä¸€å¸§
                    'spot_count': spot_result.get('all_spot_count', 0),
                    'spot_density': spot_result.get('all_spot_density', 0.0),
                    'image_shape': roi_image.shape,
                    'roi_file': roi_file
                }
                
                results.append(result)
                
                # ä¿å­˜å¯è§†åŒ–ï¼ˆæŒ‰é‡‡æ ·é—´éš”ï¼‰
                if self.viz_interval and i % self.viz_interval == 0:
                    self._save_spot_visualization(
                        roi_image, spot_result, frame_num, viz_dir
                    )
                    viz_count += 1
                    
            except Exception as e:
                print(f"\nåˆ†æROIå›¾åƒ {roi_file} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"\næ–‘å—åˆ†æå®Œæˆï¼ŒæˆåŠŸåˆ†æ {len(results)} ä¸ªROIå›¾åƒ")
        if self.viz_interval:
            print(f"å·²ä¿å­˜å¯è§†åŒ–: {viz_count} å¼ ï¼ˆé‡‡æ ·é—´éš”: {self.viz_interval}ï¼‰")
        
        return results
    
    def _save_spot_visualization(self, roi_image: np.ndarray, spot_result: dict, 
                                frame_num: int, viz_dir: str):
        """
        ä¿å­˜æ–‘å—æ£€æµ‹å¯è§†åŒ–ç»“æœ
        
        Args:
            roi_image: ROIå›¾åƒ
            spot_result: æ–‘å—æ£€æµ‹ç»“æœ
            frame_num: å¸§å·
            viz_dir: å¯è§†åŒ–è¾“å‡ºç›®å½•
        """
        try:
            # åˆ›å»ºå½©è‰²å›¾åƒç”¨äºæ ‡æ³¨
            vis_image = cv2.cvtColor(roi_image, cv2.COLOR_GRAY2RGB)
            
            # è·å–æ‰€æœ‰æ–‘å—çš„è½®å»“
            if 'all_spot_contours' in spot_result and spot_result['all_spot_contours']:
                # ç»˜åˆ¶æ–‘å—è½®å»“
                cv2.drawContours(vis_image, spot_result['all_spot_contours'], -1, 
                               (0, 255, 0), 2)  # ç»¿è‰²è½®å»“
                
                # æ ‡æ³¨æ–‘å—ä¸­å¿ƒå’Œç¼–å·
                for idx, contour in enumerate(spot_result['all_spot_contours'], 1):
                    M = cv2.moments(contour)
                    if M['m00'] != 0:
                        cx = int(M['m10'] / M['m00'])
                        cy = int(M['m01'] / M['m00'])
                        # ç»˜åˆ¶ä¸­å¿ƒç‚¹
                        cv2.circle(vis_image, (cx, cy), 3, (255, 0, 0), -1)  # è“è‰²ä¸­å¿ƒç‚¹
                        # æ ‡æ³¨ç¼–å·ï¼ˆåªæ ‡æ³¨å‰50ä¸ªï¼Œé¿å…å¤ªå¯†é›†ï¼‰
                        if idx <= 50:
                            cv2.putText(vis_image, str(idx), (cx+5, cy-5),
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            spot_count = spot_result.get('all_spot_count', 0)
            spot_density = spot_result.get('all_spot_density', 0.0)
            
            info_text = [
                f"Frame: {frame_num}",
                f"Spot Count: {spot_count}",
                f"Density: {spot_density:.4f}"
            ]
            
            y_offset = 30
            for text in info_text:
                cv2.putText(vis_image, text, (10, y_offset),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                y_offset += 30
            
            # ä¿å­˜å›¾åƒ
            viz_path = os.path.join(viz_dir, f"frame_{frame_num:06d}_spots.png")
            cv2.imwrite(viz_path, vis_image)
            
        except Exception as e:
            print(f"\nä¿å­˜å¯è§†åŒ–å¤±è´¥ frame {frame_num}: {e}")
    
    def apply_smoothing_filters(self, data: List[Dict[str, Any]], 
                               smoothing_method: str = 'gaussian',
                               window_size: int = 50,
                               sigma: float = 10.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        å¯¹æ—¶é—´åºåˆ—æ•°æ®åº”ç”¨å¹³æ»‘æ»¤æ³¢
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            smoothing_method: å¹³æ»‘æ–¹æ³• ('gaussian', 'moving_avg', 'savgol', 'median')
            window_size: æ»¤æ³¢çª—å£å¤§å°
            sigma: é«˜æ–¯æ»¤æ³¢çš„æ ‡å‡†å·®
            
        Returns:
            æ—¶é—´åºåˆ—ã€å¹³æ»‘åçš„æ–‘å—æ•°é‡ã€å¹³æ»‘åçš„æ–‘å—å¯†åº¦
        """
        time_seconds = np.array([d['time_seconds'] for d in data])
        spot_counts = np.array([d['spot_count'] for d in data])
        spot_densities = np.array([d['spot_density'] for d in data])
        
        if smoothing_method == 'gaussian':
            # é«˜æ–¯æ»¤æ³¢
            smoothed_counts = gaussian_filter1d(spot_counts, sigma=sigma)
            smoothed_densities = gaussian_filter1d(spot_densities, sigma=sigma)
            
        elif smoothing_method == 'moving_avg':
            # ç§»åŠ¨å¹³å‡æ»¤æ³¢
            smoothed_counts = np.convolve(spot_counts, np.ones(window_size)/window_size, mode='same')
            smoothed_densities = np.convolve(spot_densities, np.ones(window_size)/window_size, mode='same')
            
        elif smoothing_method == 'savgol':
            # Savitzky-Golayæ»¤æ³¢
            window_length = min(window_size, len(spot_counts))
            if window_length % 2 == 0:
                window_length -= 1
            smoothed_counts = signal.savgol_filter(spot_counts, window_length, 3)
            smoothed_densities = signal.savgol_filter(spot_densities, window_length, 3)
            
        elif smoothing_method == 'median':
            # ä¸­å€¼æ»¤æ³¢
            smoothed_counts = signal.medfilt(spot_counts, kernel_size=window_size)
            smoothed_densities = signal.medfilt(spot_densities, kernel_size=window_size)
            
        else:
            # é»˜è®¤ä½¿ç”¨é«˜æ–¯æ»¤æ³¢
            smoothed_counts = gaussian_filter1d(spot_counts, sigma=sigma)
            smoothed_densities = gaussian_filter1d(spot_densities, sigma=sigma)
        
        return time_seconds, smoothed_counts, smoothed_densities
    
    def create_temporal_plots(self, data: List[Dict[str, Any]], output_dir: str = "output",
                            smoothing_method: str = 'gaussian', window_size: int = 50, sigma: float = 10.0):
        """
        åˆ›å»ºæ—¶é—´åºåˆ—å›¾è¡¨ï¼ˆå¸¦å¹³æ»‘æ»¤æ³¢ï¼‰
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            output_dir: è¾“å‡ºç›®å½•
            smoothing_method: å¹³æ»‘æ–¹æ³•
            window_size: æ»¤æ³¢çª—å£å¤§å°
            sigma: é«˜æ–¯æ»¤æ³¢æ ‡å‡†å·®
        """
        if not data:
            print("æ²¡æœ‰æ•°æ®å¯ä»¥ç»˜åˆ¶")
            return
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        font_success = setup_chinese_font()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # æå–åŸå§‹æ•°æ®
        time_seconds = np.array([d['time_seconds'] for d in data])
        spot_counts = np.array([d['spot_count'] for d in data])
        spot_densities = np.array([d['spot_density'] for d in data])
        
        # åº”ç”¨å¹³æ»‘æ»¤æ³¢
        _, smoothed_counts, smoothed_densities = self.apply_smoothing_filters(
            data, smoothing_method, window_size, sigma)
        
        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))
        
        # æ–‘å—æ•°é‡éšæ—¶é—´å˜åŒ–ï¼ˆåŸå§‹æ•°æ®+å¹³æ»‘æ›²çº¿ï¼‰
        ax1.plot(time_seconds, spot_counts, 'b-', linewidth=0.8, alpha=0.3, label='åŸå§‹æ•°æ®')
        ax1.plot(time_seconds, smoothed_counts, 'b-', linewidth=2.5, alpha=0.9, label='å¹³æ»‘æ›²çº¿')
        ax1.fill_between(time_seconds, smoothed_counts, alpha=0.3, color='blue')
        ax1.set_xlabel('æ—¶é—´ (ç§’)' if font_success else 'Time (seconds)')
        ax1.set_ylabel('æ–‘å—æ•°é‡' if font_success else 'Spot Count')
        ax1.set_title('æ–‘å—æ•°é‡éšæ—¶é—´å˜åŒ– (å¹³æ»‘æ»¤æ³¢)' if font_success else 'Spot Count Over Time (Smoothed)')
        ax1.grid(True, alpha=0.3)
        ax1.set_xlim(0, max(time_seconds))
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_count = np.mean(spot_counts)
        max_count = np.max(smoothed_counts)
        ax1.axhline(y=mean_count, color='red', linestyle='--', alpha=0.7, 
                   label=f'å¹³å‡å€¼: {mean_count:.1f}')
        ax1.legend()
        
        # æ–‘å—å¯†åº¦éšæ—¶é—´å˜åŒ–ï¼ˆåŸå§‹æ•°æ®+å¹³æ»‘æ›²çº¿ï¼‰
        ax2.plot(time_seconds, spot_densities, 'r-', linewidth=0.8, alpha=0.3, label='åŸå§‹æ•°æ®')
        ax2.plot(time_seconds, smoothed_densities, 'r-', linewidth=2.5, alpha=0.9, label='å¹³æ»‘æ›²çº¿')
        ax2.fill_between(time_seconds, smoothed_densities, alpha=0.3, color='red')
        ax2.set_xlabel('æ—¶é—´ (ç§’)' if font_success else 'Time (seconds)')
        ax2.set_ylabel('æ–‘å—å¯†åº¦' if font_success else 'Spot Density')
        ax2.set_title('æ–‘å—å¯†åº¦éšæ—¶é—´å˜åŒ– (å¹³æ»‘æ»¤æ³¢)' if font_success else 'Spot Density Over Time (Smoothed)')
        ax2.grid(True, alpha=0.3)
        ax2.set_xlim(0, max(time_seconds))
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_density = np.mean(spot_densities)
        max_density = np.max(smoothed_densities)
        ax2.axhline(y=mean_density, color='blue', linestyle='--', alpha=0.7,
                   label=f'å¹³å‡å€¼: {mean_density:.4f}')
        ax2.legend()
        
        # æ·»åŠ æ»¤æ³¢æ–¹æ³•è¯´æ˜
        method_names = {
            'gaussian': 'é«˜æ–¯æ»¤æ³¢',
            'moving_avg': 'ç§»åŠ¨å¹³å‡',
            'savgol': 'Savitzky-Golayæ»¤æ³¢',
            'median': 'ä¸­å€¼æ»¤æ³¢'
        }
        method_name = method_names.get(smoothing_method, smoothing_method)
        fig.suptitle(f'å¹³æ»‘æ–¹æ³•: {method_name} (Ïƒ={sigma}, çª—å£={window_size})', fontsize=12, y=0.98)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.95)
        
        # ä¿å­˜å›¾è¡¨
        plot_path = os.path.join(output_dir, "spot_temporal_analysis_smoothed.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"å¹³æ»‘æ—¶é—´åºåˆ—å›¾è¡¨å·²ä¿å­˜: {plot_path}")
        
        # åˆ›å»ºç»Ÿè®¡æ‘˜è¦å›¾
        self.create_statistics_summary(data, output_dir)
        
        return plot_path
    
    def create_statistics_summary(self, data: List[Dict[str, Any]], output_dir: str):
        """
        åˆ›å»ºç»Ÿè®¡æ‘˜è¦å›¾è¡¨
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        if not data:
            return
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        font_success = setup_chinese_font()
        
        # æå–æ•°æ®
        spot_counts = [d['spot_count'] for d in data]
        spot_densities = [d['spot_density'] for d in data]
        
        # åˆ›å»ºç»Ÿè®¡å›¾è¡¨
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # æ–‘å—æ•°é‡åˆ†å¸ƒç›´æ–¹å›¾
        ax1.hist(spot_counts, bins=30, alpha=0.7, color='blue', edgecolor='black')
        ax1.set_xlabel('æ–‘å—æ•°é‡' if font_success else 'Spot Count')
        ax1.set_ylabel('é¢‘æ¬¡' if font_success else 'Frequency')
        ax1.set_title('æ–‘å—æ•°é‡åˆ†å¸ƒ' if font_success else 'Spot Count Distribution')
        ax1.grid(True, alpha=0.3)
        
        # æ–‘å—å¯†åº¦åˆ†å¸ƒç›´æ–¹å›¾
        ax2.hist(spot_densities, bins=30, alpha=0.7, color='red', edgecolor='black')
        ax2.set_xlabel('æ–‘å—å¯†åº¦' if font_success else 'Spot Density')
        ax2.set_ylabel('é¢‘æ¬¡' if font_success else 'Frequency')
        ax2.set_title('æ–‘å—å¯†åº¦åˆ†å¸ƒ' if font_success else 'Spot Density Distribution')
        ax2.grid(True, alpha=0.3)
        
        # æ–‘å—æ•°é‡ä¸å¯†åº¦çš„æ•£ç‚¹å›¾
        ax3.scatter(spot_counts, spot_densities, alpha=0.6, color='green')
        ax3.set_xlabel('æ–‘å—æ•°é‡' if font_success else 'Spot Count')
        ax3.set_ylabel('æ–‘å—å¯†åº¦' if font_success else 'Spot Density')
        ax3.set_title('æ–‘å—æ•°é‡ä¸å¯†åº¦å…³ç³»' if font_success else 'Spot Count vs Density')
        ax3.grid(True, alpha=0.3)
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = np.corrcoef(spot_counts, spot_densities)[0, 1]
        ax3.text(0.05, 0.95, f'ç›¸å…³ç³»æ•°: {correlation:.3f}', 
                transform=ax3.transAxes, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        # ç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
        ax4.axis('off')
        stats_text = f"""
ç»Ÿè®¡æ‘˜è¦:

æ–‘å—æ•°é‡:
  å¹³å‡å€¼: {np.mean(spot_counts):.2f}
  æ ‡å‡†å·®: {np.std(spot_counts):.2f}
  æœ€å°å€¼: {np.min(spot_counts)}
  æœ€å¤§å€¼: {np.max(spot_counts)}
  ä¸­ä½æ•°: {np.median(spot_counts):.2f}

æ–‘å—å¯†åº¦:
  å¹³å‡å€¼: {np.mean(spot_densities):.6f}
  æ ‡å‡†å·®: {np.std(spot_densities):.6f}
  æœ€å°å€¼: {np.min(spot_densities):.6f}
  æœ€å¤§å€¼: {np.max(spot_densities):.6f}
  ä¸­ä½æ•°: {np.median(spot_densities):.6f}

æ•°æ®ç‚¹æ€»æ•°: {len(data)}
        """
        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜ç»Ÿè®¡å›¾è¡¨
        stats_path = os.path.join(output_dir, "spot_statistics_summary.png")
        plt.savefig(stats_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ç»Ÿè®¡æ‘˜è¦å›¾è¡¨å·²ä¿å­˜: {stats_path}")
    
    def save_data_to_csv(self, data: List[Dict[str, Any]], output_dir: str = "output"):
        """
        ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶
        
        Args:
            data: æ–‘å—åˆ†ææ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        if not data:
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(data)
        
        # ä¿å­˜CSVæ–‡ä»¶
        csv_path = os.path.join(output_dir, "spot_temporal_data.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8')
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {csv_path}")
        
        # ä¿å­˜JSONæ ¼å¼
        json_path = os.path.join(output_dir, "spot_temporal_data.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ°JSONæ–‡ä»¶: {json_path}")
    
    def run_analysis(self, roi_dir: str = "data/roi_imgs", output_dir: str = "output",
                    smoothing_method: str = 'gaussian', window_size: int = 50, sigma: float = 10.0):
        """
        è¿è¡Œå®Œæ•´çš„æ–‘å—æ—¶é—´åºåˆ—åˆ†æï¼ˆå¸¦å¹³æ»‘æ»¤æ³¢ï¼‰
        
        Args:
            roi_dir: ROIå›¾åƒç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            smoothing_method: å¹³æ»‘æ–¹æ³• ('gaussian', 'moving_avg', 'savgol', 'median')
            window_size: æ»¤æ³¢çª—å£å¤§å°
            sigma: é«˜æ–¯æ»¤æ³¢æ ‡å‡†å·®
        """
        print("\n" + "="*80)
        print("ROIæ–‘å—æ—¶é—´åºåˆ—åˆ†æ (æ•´ä¸ªROIåŒºåŸŸï¼Œä¸åŒºåˆ†æ’•è£‚é¢å’Œå‰ªåˆ‡é¢)")
        print("="*80)
        print(f"ROIç›®å½•: {roi_dir}")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        print(f"å¹³æ»‘æ–¹æ³•: {smoothing_method}")
        if self.viz_interval:
            print(f"å¯è§†åŒ–é—´éš”: æ¯ {self.viz_interval} å¸§")
        
        # åˆ†æROIå›¾åƒçš„æ–‘å—ç‰¹å¾
        data = self.analyze_roi_spots(roi_dir, output_dir)
        
        if not data:
            print("æ²¡æœ‰å¯åˆ†æçš„æ•°æ®")
            return
        
        # åˆ›å»ºæ—¶é—´åºåˆ—å›¾è¡¨ï¼ˆå¸¦å¹³æ»‘æ»¤æ³¢ï¼‰
        plot_path = self.create_temporal_plots(data, output_dir, smoothing_method, window_size, sigma)
        
        # ä¿å­˜æ•°æ®
        self.save_data_to_csv(data, output_dir)
        
        # è¾“å‡ºç»Ÿè®¡æ‘˜è¦
        self.print_statistics_summary(data)
        
        print(f"\n{'='*80}")
        print("âœ… æ–‘å—æ—¶é—´åºåˆ—åˆ†æå®Œæˆï¼")
        print(f"{'='*80}")
        print(f"ğŸ“Š å¹³æ»‘å›¾è¡¨: {plot_path}")
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {output_dir}")
        print(f"ğŸ”§ å¹³æ»‘æ–¹æ³•: {smoothing_method}, çª—å£å¤§å°: {window_size}, Ïƒ: {sigma}")
        if self.viz_interval:
            print(f"ğŸ–¼ï¸  å¯è§†åŒ–: {output_dir}/spot_visualizations/")
        
        return data
    
    def print_statistics_summary(self, data: List[Dict[str, Any]]):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        if not data:
            return
        
        spot_counts = [d['spot_count'] for d in data]
        spot_densities = [d['spot_density'] for d in data]
        
        print("\n=== æ–‘å—æ—¶é—´åºåˆ—ç»Ÿè®¡æ‘˜è¦ ===")
        print(f"æ•°æ®ç‚¹æ€»æ•°: {len(data)}")
        print(f"æ—¶é—´è·¨åº¦: {data[0]['time_seconds']:.1f} - {data[-1]['time_seconds']:.1f} ç§’")
        print(f"å¸§æ•°èŒƒå›´: {data[0]['frame_num']} - {data[-1]['frame_num']}")
        
        print("\næ–‘å—æ•°é‡ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {np.mean(spot_counts):.2f}")
        print(f"  æ ‡å‡†å·®: {np.std(spot_counts):.2f}")
        print(f"  æœ€å°å€¼: {np.min(spot_counts)}")
        print(f"  æœ€å¤§å€¼: {np.max(spot_counts)}")
        print(f"  ä¸­ä½æ•°: {np.median(spot_counts):.2f}")
        
        print("\næ–‘å—å¯†åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {np.mean(spot_densities):.6f}")
        print(f"  æ ‡å‡†å·®: {np.std(spot_densities):.6f}")
        print(f"  æœ€å°å€¼: {np.min(spot_densities):.6f}")
        print(f"  æœ€å¤§å€¼: {np.max(spot_densities):.6f}")
        print(f"  ä¸­ä½æ•°: {np.median(spot_densities):.6f}")
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = np.corrcoef(spot_counts, spot_densities)[0, 1]
        print(f"\næ–‘å—æ•°é‡ä¸å¯†åº¦ç›¸å…³ç³»æ•°: {correlation:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ROIæ–‘å—æ—¶é—´åºåˆ—åˆ†æå·¥å…· - ç»Ÿè®¡æ•´ä¸ªROIåŒºåŸŸçš„æ–‘å—æ•°é‡å’Œå¯†åº¦å˜åŒ–',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³•
  python analyze_spot_temporal.py --roi_dir data/roi_imgs --output_dir output/spot_analysis
  
  # è‡ªå®šä¹‰å¹³æ»‘å‚æ•°ï¼ˆSavitzky-Golayæ»¤æ³¢ï¼‰
  python analyze_spot_temporal.py --roi_dir data/roi_imgs --output_dir output \
    --smoothing_method savgol --window_size 100
  
  # ä¿å­˜æ¯ä¸€å¸§çš„å¯è§†åŒ–ï¼ˆæ¯50å¸§ï¼‰
  python analyze_spot_temporal.py --roi_dir data/roi_imgs --output_dir output \
    --viz_interval 50
  
  # ä½¿ç”¨é«˜æ–¯æ»¤æ³¢+è‡ªå®šä¹‰Ïƒå€¼
  python analyze_spot_temporal.py --roi_dir data/roi_imgs --output_dir output \
    --smoothing_method gaussian --sigma 15.0
  
  # æ‰¹é‡å¤„ç†å¤šä¸ªè§†é¢‘
  python analyze_spot_temporal.py --roi_dir video1/roi_imgs --output_dir video1/spot_analysis
  python analyze_spot_temporal.py --roi_dir video2/roi_imgs --output_dir video2/spot_analysis --viz_interval 100
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--roi_dir', type=str, default='data/roi_imgs',
                       help='ROIå›¾åƒç›®å½•è·¯å¾„ (é»˜è®¤: data/roi_imgs)')
    parser.add_argument('--output_dir', type=str, default='output/spot_temporal',
                       help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: output/spot_temporal)')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--smoothing_method', type=str, default='gaussian',
                       choices=['gaussian', 'moving_avg', 'savgol', 'median'],
                       help='å¹³æ»‘æ–¹æ³•: gaussian=é«˜æ–¯æ»¤æ³¢, moving_avg=ç§»åŠ¨å¹³å‡, '
                            'savgol=Savitzky-Golayæ»¤æ³¢, median=ä¸­å€¼æ»¤æ³¢ (é»˜è®¤: gaussian)')
    parser.add_argument('--window_size', type=int, default=50,
                       help='æ»¤æ³¢çª—å£å¤§å° (é»˜è®¤: 50)')
    parser.add_argument('--sigma', type=float, default=10.0,
                       help='é«˜æ–¯æ»¤æ³¢çš„æ ‡å‡†å·®Ïƒ (é»˜è®¤: 10.0)')
    parser.add_argument('--viz_interval', type=int, default=None,
                       help='å¯è§†åŒ–é‡‡æ ·é—´éš”ï¼Œæ¯éš”å¤šå°‘å¸§ä¿å­˜ä¸€æ¬¡æ–‘å—æ ‡æ³¨å›¾ (é»˜è®¤: None=ä¸ä¿å­˜)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(args.roi_dir):
        print(f"é”™è¯¯: ROIç›®å½•ä¸å­˜åœ¨: {args.roi_dir}")
        return 1
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = SpotTemporalAnalyzer(viz_interval=args.viz_interval)
    
    # è¿è¡Œåˆ†æ
    data = analyzer.run_analysis(
        roi_dir=args.roi_dir,
        output_dir=args.output_dir,
        smoothing_method=args.smoothing_method,
        window_size=args.window_size,
        sigma=args.sigma
    )
    
    if data:
        print(f"\nğŸ¯ åˆ†æå®Œæˆï¼å…±åˆ†æäº† {len(data)} ä¸ªæ—¶é—´ç‚¹çš„æ–‘å—æ•°æ®")
        print("ğŸ“ˆ ç”Ÿæˆäº†å¹³æ»‘æ—¶é—´åºåˆ—æ›²çº¿å›¾å’Œç»Ÿè®¡æ‘˜è¦")
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())
